<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter version="5.0" xml:id="cha.deploy.install"
 xmlns="http://docbook.org/ns/docbook"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink">
 <info>
  <title>Preparing Installation on Physical Machines or Private Cloud</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker/>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>
 <para>
  This chapter prepares the installation of &productname; on physical
  machines, manually configured virtual machines and in private cloud
  environments.
 </para>

 <para>
  There are two ways to build a cluster, one of which has two variants.
 </para>

 <orderedlist>
  <listitem>
   <para>
    Installing nodes from installation media (for virtual machines, directly
    from ISO images, or physical media for bare-metal installations).
   </para>
   <warning>
    <title>&Admin_Node;</title>
    <para>
     You must install the &admin_node; first and configure it using
     &dashboard; before installing the other nodes in the cluster.
    </para>
   </warning>
   <para>
    With this method, there are two ways to install the worker nodes:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Individually, choosing the node type during installation. This is only
      feasible for a small number of workers.
     </para>
    </listitem>
    <listitem>
     <para>
      Installing the worker nodes using &ay; so that they receive their
      configuration automatically from the &admin_node;. This is the
      recommended method for larger clusters, but it is equally suitable for
      small clusters.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    With both methods, the procedure to install the &admin_node; is identical.
    Only installation process for the &worker_node;s differs.
   </para>
  </listitem>
  <listitem>
   <para>
    Using preinstalled virtual disk images. &suse; offers
    <literal>QCOW2</literal> images for clusters hosted on KVM or &xen;
    hypervisors.
   </para>
   <tip>
    <title>Use Disk Images</title>
    <para>
     When building a cluster from virtual machines, &suse; recommends building
     nodes from pre-installed disk images, rather than installing new instances
     from an ISO image.
    </para>
    <para>
     Virtual-disk images are available for Hyper-V, KVM, OpenStack, VMware,
     and Xen.
    </para>
    <para>
     These VM images include the relevant hypervisor's guest additions or tools
     (where this is applicable).
    </para>
   </tip>
  </listitem>
 </orderedlist>

 <note>
  <para>
   It is possible to start setup using PXE. For the full procedure, refer to
   the &sle; 12 Deployment Guide:
   <link xlink:href="https://www.suse.com/documentation/sles-12/singlehtml/book_sle_deployment/book_sle_deployment.html#cha.deployment.prep_boot"/>.
  </para>
  <para>
   You can directly use the <filename>initrd</filename> and
   <filename>linux</filename> files from your install media, or install the
   package <package>tftpboot-installation-CAASP-3.0</package> onto your TFTP
   server. The package provides the required <filename>initrd</filename> and
   <filename>linux</filename> files in the <filename>/srv/tftpboot/</filename>
   directory. You will need to modify the paths used in the &sle; 12
   Deployment Guide to correctly point to the files provided by the package.
  </para>
 </note>

 <sect1 xml:id="sec.deploy.install.overview">
  <title>Installing from USB, DVD or ISO Images</title>
  <para>
   This procedure provides a short overview of the required steps for
   the full cluster setup.
  </para>
  <procedure>
   <step>
    <para>
     Choose an installation source. You can install from DVD or USB
     sticks. On virtual machines, you can install from ISO images.
    </para>
   </step>
   <step>
    <para>
     Install and configure the &Admin_Node;. For details, see <xref
     linkend="sec.deploy.nodes.admin_install" /> and <xref
     linkend="sec.deploy.nodes.admin_configuration" />.
    </para>
   </step>
   <step>
    <para>
     Install master and worker nodes. To automate the installation, you
     can use an &ay; file provided by the &admin_node;. For details, see
     <xref linkend="sec.deploy.nodes.worker_install" />.
    </para>
   </step>
   <step>
    <para>
     Finish the installation by bootstrapping the cluster. For details,
     see <xref linkend="sec.deploy.install.bootstrap" />.
    </para>
   </step>
  </procedure>
 </sect1>

 <sect1 xml:id="sec.deploy.install.qcow2">
  <title>Installing from Disk Images</title>
  <para>
   For building clusters from virtual machines on supported
   hypervisors, it is not necessary to individually install each
   node. &suse; offers pre-installed VM disk images in the following
   formats:
  </para>
  <variablelist>
   <varlistentry>
    <term>&kvm; and &xen;</term>
    <listitem>
     <para>
      In QCOW2 format, for &kvm; and for &xen; using full virtualization.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>&xen;</term>
    <listitem>
     <para>
      In QCOW2 format, for &xen; using paravirtualization.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>VMware</term>
    <listitem>
     <para>
      In VMDK format, for VMware ESXi.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     VHD
    </term>
    <listitem>
     <para>For Microsoft Hyper-V.</para>
    </listitem>
   </varlistentry>
  </variablelist>

  <sect2 xml:id="sec.deploy.install.qcow2.overview">
   <title>Overview</title>
   <para>
    When deploying a cluster node from pre-installed disk images, the setup
    program never runs. Thus, it is not possible to set values such as time
    servers, node names, the &rootuser; password for the &admin_node; and so
    on in the normal way.
   </para>
   <para>
    Instead, you need to set these values using <literal>cloud-init</literal> configuration
    files. You should place these into a specific subdirectory, then make an
    ISO image. Attach this to the VM as a secondary disk. The pre-installed
    operating system on the disk image looks for a medium with the
    volume label <literal>cidata</literal>, searches that volume for the
    configuration files, and applies the settings within them.
   </para>
   <para>
    Full details of the cloud-init metadata files can be found in the
    Configuration chapter of this manual; for more information, see
    <xref linkend="sec.deploy.cloud-init"/>.
   </para>
   <para>
    As with individual node installation, the &admin_node; needs to be
    configured separately and differently from the &worker_node;s.
   </para>
  </sect2>

  <sect2 xml:id="sec.deploy.install.qcow2.procedure.openstack">
   <title>Installation on &soc;</title>
   <para>
    You can deploy a &productname; on &soc; using &ostack;.
    You will need a &productname; machine image and &ostack; Heat templates.
    Once you have created a stack, you will continue with the &productname; setup.
   </para>
   <note>
    <title>&productname; machine image for &soc;</title>
    <para>
     Download the latest &productname; for &ostack; image from
     <link xlink:href="https://download.suse.com">https://download.suse.com</link>
     (for example,
     <filename>SUSE-CaaS-Platform-3.0-OpenStack-Cloud.x86_64-1.0.0-GM.qcow2</filename>).
    </para>
   </note>
    <note>
     <title>
      &ostack; Heat Templates Repository
     </title>
     <para>
      &productname; Heat templates are available from <link xlink:href="https://github.com/SUSE/caasp-openstack-heat-templates">GitHub</link>.
     </para>
   </note>

   <sect3>
    <title>Using the Horizon Dashboard</title>
    <procedure>
    <step>
     <para>
      Go to <guimenu>Project &rarr; Compute &rarr; Images</guimenu> and click on
      <guimenu>Create Image</guimenu>.
     </para>
     <para>
      Give your image a name (for example: <literal>CaaSP-3</literal>); you will
      need to use this later to find the image.
     </para>
    </step>
     <step>
      <para>
       Go to <guimenu>Project &rarr; Orchestration</guimenu> and click on
       <guimenu>Stacks</guimenu>.
      </para>
      <informalfigure>
       <mediaobject>
        <imageobject role="fo">
         <imagedata fileref="horizon_stacks.png" width="100%"/>
        </imageobject>
        <imageobject role="html">
         <imagedata fileref="horizon_stacks.png" width="100%"/>
        </imageobject>
       </mediaobject>
      </informalfigure>
     </step>
     <step>
      <para>
       Click on <guimenu>Launch Stack</guimenu> and provide the stack
       templates. Either upload the files, provide the URL to the raw files
       directly (only applies to stack template), or copy and paste the
       contents into the <guimenu>Direct Input</guimenu> fields.
      </para>
      <warning>
       <title>Replace the default <literal>root_password</literal></title>
        <para>
         Do not use the <filename>caasp-environment.yaml</filename> directly
         from the GitHub repository.
        </para>
         <para>
          You must make sure to replace the value for
          <literal>root_password</literal> with a secure password.
          This will become the password for the &rootuser; account on all
          nodes in the stack.
         </para>
     </warning>
      <informalfigure>
       <mediaobject>
        <imageobject role="fo">
         <imagedata fileref="horizon_launch_stack.png" width="100%"/>
        </imageobject>
        <imageobject role="html">
         <imagedata fileref="horizon_launch_stack.png" width="100%"/>
        </imageobject>
       </mediaobject>
      </informalfigure>
     </step>
    <step>
     <para>
      Click <guimenu>Next</guimenu>.
     </para>
    </step>
     <step>
      <para>
       Now you need to define more information about the stack.
      </para>
      <variablelist>
       <varlistentry>
        <term><literal>Stack Name</literal></term>
        <listitem>
         <para>
          Give your stack a name
         </para>
        </listitem>
       </varlistentry>
       <varlistentry>
        <term><literal>Password</literal></term>
        <listitem>
         <para>
          Your &soc; password
         </para>
        </listitem>
       </varlistentry>
       <varlistentry>
        <term><literal>Image</literal></term>
        <listitem>
         <para>
          Select the image your machines will be created from
         </para>
        </listitem>
       </varlistentry>
       <varlistentry>
        <term><literal>root_password</literal></term>
        <listitem>
         <para>
          Set the root password for your cluster machines
         </para>
        </listitem>
       </varlistentry>
       <varlistentry>
        <term><literal>admin</literal>/master/worker_flavor</term>
        <listitem>
         <para>
          Select the machine flavor for your nodes
         </para>
        </listitem>
       </varlistentry>
       <varlistentry>
        <term><literal>worker_count</literal></term>
        <listitem>
         <para>
          Number of worker nodes to be launched
         </para>
        </listitem>
       </varlistentry>
       <varlistentry>
        <term><literal>external_net</literal></term>
        <listitem>
         <para>
          Select an external network that your cluster will be reachable from
         </para>
        </listitem>
       </varlistentry>
       <varlistentry>
        <term><literal>internal_net_cidr</literal></term>
        <listitem>
         <para>
          The internal network range to be used inside the cluster
         </para>
        </listitem>
       </varlistentry>
       <varlistentry>
        <term><literal>dns_nameserver</literal></term>
        <listitem>
         <para>
          Internal name server for the cluster
         </para>
        </listitem>
       </varlistentry>
      </variablelist>
      <informalfigure>
       <mediaobject>
        <imageobject role="fo">
         <imagedata fileref="horizon_stack_options.png" width="100%"/>
        </imageobject>
        <imageobject role="html">
         <imagedata fileref="horizon_stack_options.png" width="100%"/>
        </imageobject>
       </mediaobject>
      </informalfigure>
     </step>
     <step>
      <para>
      Click <guimenu>Launch</guimenu>.
      </para>
     </step>
     <step>
      <para>
       After the cluster has been started and the cluster overview shows
       <guimenu>Create Complete</guimenu>, you need to find the external IP
       address for the admin node of your cluster (here:
       <literal>10.86.1.72</literal>). Now visit that IP address in your browser.
       You should see the &dashboard; login page and can continue with
       <xref linkend="sec.deploy.nodes.admin_configuration"/>.
      </para>
      <informalfigure>
       <mediaobject>
        <imageobject role="fo">
         <imagedata fileref="horizon_stack_resources.png" width="100%"/>
        </imageobject>
        <imageobject role="html">
         <imagedata fileref="horizon_stack_resources.png" width="100%"/>
        </imageobject>
       </mediaobject>
      </informalfigure>
     </step>
    </procedure>
   </sect3>

   <sect3>
    <title>Using the &ostack; CLI</title>
    <note>
     <para>
      You need to have access to the &ostack; command-line tools. You can
      either access those via <literal>ssh</literal> on your &soc; admin
      server or
      <link xlink:href="https://docs.openstack.org/newton/user-guide/common/cli-install-openstack-command-line-clients.html">
      install a local <command>openstack</command> client</link>.
     </para>
     <para>
      To use the local client, you need to access <guimenu>Project &rarr; Compute &rarr;
      Access &amp; Security</guimenu> in the Horizon Dashboard and click on the
      <guimenu>Download &ostack; RC File v3</guimenu>.
     </para>
     <para>
      The downloaded file is a script that you then need to load using the
      <command>source</command> command. The script will ask you for your
      &soc; password.
     </para>
      <screen>&prompt.user;<command>source container-openrc.sh</command></screen>
    </note>
    <procedure>
      <step>
       <para>
        Upload the container image to &ostack; Glance (Image service). This
        example uses the name <literal>CaaSP-3</literal> as the name of the
        image that is created in &soc;.
       </para>
      <screen>&prompt.user;<command>openstack image create --public --disk-format qcow2 \
--container-format bare \
--file SUSE-CaaS-Platform-3.0-OpenStack-Cloud.x86_64-1.0.0-GM.qcow2 \
<replaceable>CaaSP-3</replaceable></command></screen>
      </step>
      <step>
       <warning>
        <title>Replace the default <literal>root_password</literal></title>
         <para>
          Do not use the <filename>caasp-environment.yaml</filename> directly
          from the GitHub repository.
         </para>
         <para>
          You must make sure to replace the value for
          <literal>root_password</literal> with a secure password.
          This will become the password for the &rootuser; account on all
          nodes in the stack.
         </para>
      </warning>
       <para>
        Download the <filename>caasp-stack.yaml</filename> and
        <filename>caasp-environment.yaml</filename> Heat templates to your
        workstation and then run the <command>openstack stack create</command>
        command.
       </para>
       <screen>&prompt.user;<command>openstack stack create \
-t caasp-stack.yaml \
-e caasp-environment.yaml \
--parameter image=<replaceable>CaaSP-3</replaceable> <replaceable>caasp3-stack</replaceable></command></screen>
      </step>

     <step>
      <para>
       Find out which (external) IP address was assigned to the admin node
       of your &productname; cluster (here: <literal>10.81.1.51</literal>).
      </para>
      <screen>&prompt.user;<command>openstack server list --name "admin" | awk 'FNR > 3 {print $4 $5 $9}'</command>
caasp3-stack-admin|10.81.1.51</screen>
      </step>
      <step>
       <para>
        Visit the external IP address in your browser. You should see the &dashboard;
        login page and can continue with <xref linkend="sec.deploy.nodes.admin_configuration"/>.
       </para>
     </step>
    </procedure>
   </sect3>
  </sect2>

  <sect2 xml:id="sec.deploy.install.qcow2.procedure">
   <title>Procedure for Hypervisors</title>
   <para>
    The process is very similar for all hypervisors. The examples in this
    manual use &kvm; running on &sle;.
   </para>
   <para>
    You need to build at least two ISO images with configuration information:
    one for the &admin_node; and one for the &worker_node;s. If you wish to
    assign specific host names or customize individual &worker_node;s, then
    you should create separate ISO images for each node.
   </para>
   <para>
    Additionally, you need to make a separate copy of the downloaded VM disk
    image for each VM. We suggest keeping the original download elsewhere and
    making a fresh copy for each node, naming it appropriately: for example,
    <literal>caas-admin</literal>, <literal>caas-master</literal>,
    <literal>caas-worker1</literal>, <literal>caas-worker2</literal> and so
    on.
   </para>
   <sect3 xml:id="sec.deploy.install.qcow2.configuration">
    <title>Configuration Files</title>
    <para>
     There are two separate configuration files: <filename>user-data</filename>
     and <filename>meta-data</filename>. Each node needs both. Thus, you need to
     prepare at a minimum one pair of files for the &admin_node;, and another
     pair of files for all the &worker_node;s.
    </para>
    <para>
     Place the files into subdirectories named <filename>cc-admin</filename>
     for the &admin_node; and <filename>cc-worker</filename> for the
     &worker_node;s.
    </para>
    <para>
     So, for instance, if your working directory is <filename>~/cloud-config</filename>,
     then for the admin node, you need these two files:
    </para>
    <screen>~/cloud-config/cc-admin/user-data
~/cloud-config/cc-admin/meta-data</screen>
    <para>
     For a &worker_node;, you need:
    </para>
    <screen>~/cloud-config/cc-worker/user-data
~/cloud-config/cc-worker/meta-data</screen>
    <para>
     The same <filename>meta-data</filename> file can be used for both node types.
     Here is an sample <filename>meta-data</filename> file:
     </para>
     <screen>#cloud-config
instance-id: <replaceable>iid-CAAS01</replaceable>
network-interfaces: |
   auto <replaceable>eth0</replaceable>
   iface <replaceable>eth0</replaceable> inet dhcp</screen>
    <para>
     The <filename>user-data</filename> file contains settings such as time
     servers, the &rootuser; password, and the node type.
    </para>
    <para>
     Here is an example <filename>cc-admin/user-data</filename> file for an
     &admin_node;:
    </para>
    <screen>#cloud-config
debug: True
disable_root: False
ssh_deletekeys: False
ssh_pwauth: True
chpasswd:
   list: |
     root:<replaceable>MY_PASSWORD</replaceable>
     expire: False
ntp:
   servers:
     - <replaceable>ntp1.example.com</replaceable>
     - <replaceable>ntp2.example.com</replaceable>
runcmd:
   - /usr/bin/systemctl enable --now ntpd
suse_caasp:
  role: <replaceable>admin</replaceable></screen>
    <para>
     Here is an example <filename>cc-worker/user-data</filename>  for a
     &worker_node;. Rather than providing the &rootuser; password in
     clear text, you can use a hash instead; this example is hashed with
     SHA-256.
    </para>
    <screen>#cloud-config
debug: True
disable_root: False
ssh_deletekeys: False
ssh_pwauth: True
chpasswd:
   list: |
   root:<replaceable>$5$eriogqzq$Dg7PxHsKGzziuEGkZgkLvacjuEFeljJ.rLf.hZqKQLA</replaceable>
     expire: False
suse_caasp:
    role: <replaceable>cluster</replaceable>
    admin_node: <replaceable>caas-admin.example.com</replaceable></screen>
   </sect3>
   <sect3 xml:id="sec.deploy.install.qcow2.iso-image">
    <title>Preparing an ISO image</title>
    <para>
     Once you have edited the configuration files with your desired settings,
     you need to create an ISO image with the volume label <literal>cidata</literal>
     containing only the subdirectory for that node type.
    </para>
    <para>
     On &sle; 12 or &opensuse; 42, use <command>genisoimage</command> to do this.
     On &sle; or &opensuse; 15, use <command>mkisofs</command>. The parameters
     are the same for both commands.
    </para>
    <para>
     For example, to create the ISO image for an admin node on a computer
     running &opensuse; 42:
    </para>
    <screen>&prompt.user;sudo genisoimage -output <replaceable>cc-admin.img</replaceable> -volid <replaceable>cidata</replaceable> -joliet -rock <replaceable>cc-admin</replaceable></screen>
    <para>
     To create the ISO image for a worker node on a computer running &opensuse;
     15, substituting the name of the folder containing the configuration
     files for a &worker_node; and titling the volume <literal>cc-worker</literal>:
    </para>
    <screen>&prompt.user;sudo mkisofs -output <replaceable>cc-worker.img</replaceable> -volid <replaceable>cidata</replaceable> -joliet -rock <replaceable>cc-worker</replaceable></screen>
   </sect3>

   <sect3 xml:id="sec.deploy.install.qcow2.admin">
    <title>Procedure to Bring Up an &Admin_Node;</title>
    <procedure>
     <step>
      <para>
       Make a folder called <filename>cc-admin</filename>.
      </para>
     </step>
     <step>
      <para>
       In that folder, prepare <filename>user-data</filename> and <filename>meta-data</filename>
       files to set the &rootuser; password, node name and so on for the
       &admin_node;.
      </para>
     </step>
     <step>
      <para>
       Create an ISO file called <filename>cc-admin.img</filename> containing
       these two files:
      </para>
      <screen>&prompt.user;sudo genisoimage -output cc-admin.img -volid cidata -joliet -rock cc-admin</screen>
     </step>
     <step>
      <para>
       Create a new VM for the &admin_node;.
      </para>
     </step>
     <step>
      <para>
       Attach a copy of the downloaded disk image as its main hard disk.
      </para>
     </step>
     <step>
      <para>
       Attach the <filename>cc-admin.img</filename> disk image as a secondary disk.
     </para>
     </step>
     <step>
      <para>
       Start the VM.
      </para>
     </step>
     <step>
      <para>
       Configure the new &admin_node; as in step <xref linkend="sec.deploy.nodes.admin_configuration"/>.
      </para>
     </step>
    </procedure>
   </sect3>

   <sect3 xml:id="sec.deploy.install.qcow2.worker">
    <title>Procedure to Bring Up a &Worker_Node;</title>
    <para>
     The first procedure is to generate the configuration files. Unless
     you wish to customize the individual &worker_node;s, then you need only
     do this part once.
    </para>
    <procedure>
     <step>
      <para>
       Make a folder called <filename>cc-worker</filename>.
      </para>
     </step>
     <step>
      <para>
       In that folder, prepare a <filename>user-data</filename> file to set the &rootuser;
       password, network settings and so on for the &worker_node;. If no
       changes are necessary, you can copy the <filename>meta-data</filename>
       file from that used to configure the &admin_node;.
      </para>
     </step>
     <step>
      <para>
       Create an ISO file called <filename>cc-worker.img</filename> containing
       these two files:
      </para>
      <screen>&prompt.user; sudo genisoimage -output cc-worker.img -volid cidata -joliet -rock cc-worker</screen>
     </step>
    </procedure>
    <para>
     Once you have a disk image containing the configuration files, it can be
     reused for multiple &worker_node;s.
    </para>
    <para>
     Then, repeat the following steps for each &worker_node;:
    </para>
    <procedure>
     <step>
      <para>
       Create a new VM for the &worker_node;.
      </para>
     </step>
     <step>
      <para>
       Attach a copy of the downloaded disk image as its main hard disk.
      </para>
     </step>
     <step>
      <para>
       Attach the <filename>cc-worker.img</filename> disk image as a
       secondary disk.
      </para>
     </step>
     <step>
      <para>
       Start the VM.
      </para>
     </step>
    </procedure>
    <para>
     Once you have brought up as many &worker_node;s as you need, proceed to
     bootstap the cluster using the &dashboard; dashboard.
    </para>
   </sect3>
  </sect2>
 </sect1>
</chapter>
