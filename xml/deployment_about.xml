<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter version="5.0" xml:id="cha.deploy.about"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker></dm:bugtracker>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
  <title>About &productname;</title>
 </info>

  <para>
  &suse; &productname; is a Cloud-Native Computing Foundation (CNCF) certified
  &kube; distribution on top of &suse; &mos;.
  &suse; &mos; is a minimalist operating system based on &sle;, dedicated
  to hosting containers. &suse; &mos; inherits the benefits of &sle; in the
  form of a smaller, simpler, and more robust operating system, optimized for
  large, clustered deployments. It also features an atomic, transactional update
  mechanism, making the system more resilient against software-update-related
  problems.
 </para>

  <para>
  &productname; automates the orchestration and management of
  containerized applications and services with powerful &kube; capabilities,
  including:
 </para>
 <itemizedlist>
  <listitem>
   <para>
    Workload scheduling optimizing hardware utilization while taking the
    container requirements into account.
   </para>
  </listitem>
  <listitem>
   <para>
    Service proxies providing single IP addresses for services and
    distributing the load between containers.
   </para>
  </listitem>
  <listitem>
   <para>
    Up and down scaling for applications accommodating load changes.
   </para>
  </listitem>
  <listitem>
   <para>
    Non-disruptive rollout/rollback of new applications and updates enabling
    frequent changes without downtime.
   </para>
  </listitem>
  <listitem>
   <para>
    Health monitoring and management supporting application self-healing to
    ensure application availability.
   </para>
  </listitem>
 </itemizedlist>
 <para>
  In addition, &productname; simplifies the platform operatorâ€™s experience, with
  everything needed to get it up and running quickly, and to managing the
  environment effectively in production. It provides:
 </para>
 <itemizedlist>
  <listitem>
   <para>
    A complete container execution environment, including a purpose-built
    container host operating system (&suse; &mos;), container runtime, and
    container image registries.
   </para>
  </listitem>
  <listitem>
   <para>
    Enhanced data center integration features enabling you to plug &kube;
    into a new or existing infrastructure, systems, and processes.
   </para>
  </listitem>
  <listitem>
   <para>
    Application ecosystem support with &sle; container base images, and access
    to tools and services offered by &suse; Ready for CaaS Platform partners and
    the &kube; community.
   </para>
  </listitem>
  <listitem>
   <para>
    End-to-End security, implemented holistically across the full stack.
   </para>
  </listitem>
  <listitem>
   <para>
    Advanced platform management simplifying platform installation,
    configuration, re-configuration, monitoring, maintenance, updates,
    and recovery.
   </para>
  </listitem>
  <listitem>
   <para>
    Enterprise hardening including comprehensive interoperability testing,
    support for thousands of platforms, and world-class platform maintenance and
    technical support.
   </para>
  </listitem>
 </itemizedlist>

 <para>
  You can deploy &productname; on bare metal hardware, virtual machines and in
  the public cloud. It provides a highly-scalable cluster immediately ready to
  run.
 </para>
 <para>
  &productname; inherits benefits of &sle; and uses tools and technologies
  well-known to system administrators such as <literal>cloud-init</literal> and
  Salt. It adds <emphasis role="bold">transactional updates</emphasis> as a
  main innovation to the stack. A transactional update is an update that can
  be installed when the system is running without any down-time. A
  transactional update can be rolled back, so if the update fails or is not
  compatible with your infrastructure, you can easily restore the previous
  system state.
 </para>
 <para>
  The transactional update setup relies on the following features of the
  &btrfs; file system that is used on &productname;:
 </para>
 <itemizedlist>
  <listitem>
   <para>
    The root file system and its snapshots are read-only.
   </para>
  </listitem>
  <listitem>
   <para>
    Sub-volumes for data sharing are read-write.
   </para>
  </listitem>
  <listitem>
   <para>
    &productname; introduces overlays of the <literal>/etc</literal> directories
    used by <literal>cloud-init</literal> and &salt;.
   </para>
  </listitem>
 </itemizedlist>

 <para>
  For more detailed information on the product, refer to the SUSE CaaS Platform
  Release Notes at <link xlink:href="https://www.suse.com/releasenotes/"/>.
 </para>

 <sect1 xml:id="sec.deploy.architecture">
  <title>Architectural Overview</title>

  <para>
   A typical &productname; cluster consists of three node types:
  </para>

  <variablelist>
   <varlistentry>
    <!-- Admin Node -->
    <term>
     <xref xrefstyle="select:title"
           linkend="sec.deploy.architecture.administration-node"/>
    </term>
    <listitem>
     <para>
      A single node hosting the Web-bashed dashboard for managing the whole
      cluster. It is also node to provision master and worker
      nodes. Node deployment and management is handled by the configuration
      management software &salt;. The &admin_node; acts as the &smaster;
      assigning roles to the other nodes in the cluster, the  &sminion;s.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <!-- Master Node(s) -->
    <term>
     <xref xrefstyle="select:title"
           linkend="sec.deploy.architecture.master-nodes"/>
    </term>
    <listitem>
     <para>
      One or more &master_node;s managing the cluster infrastructure, like
      scheduling, pod-management and storing the cluster data.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <!-- Worker Nodes -->
    <term>
     <xref xrefstyle="select:title"
           linkend="sec.deploy.architecture.worker_node"/>
    </term>
    <listitem>
     <para>
      The &worker_node;s running the application containers with the main
      workload of the cluster.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   In large-scale clusters, you may optionally add additional nodes helping you
   to manage and run the cluster:
  </para>

  <variablelist>
   <varlistentry>
    <term>&smtool; server (&smt;)</term>
    <listitem>
     <para>
      A local &smt; server that manages subscriptions for &suse; products. It
      provides software packages, updates and patches for all &suse; products
      you are subscribed to. It regularly synchronizes with the &scc; (SCC, <link
      xlink:href="https://scc.suse.com/"/>) to get the latest updates. With a
      local &smt; server, all cluster nodes get updates from this server rather
      than from the &scc;. This decreases network traffic.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Log Host</term>
    <listitem>
     <para>
      A host storing log files for all cluster nodes, providing a single
      location for log file monitoring and analysis.
      <remark condition="clarity">
       2018-09-19 - fs: How to set up such a host? How to configure the nodes?
      </remark>
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   In addition to these servers, container data storage needs to be provided by
   either the &ses; product or a server exporting NFS shares.
  </para>

  <para>
   The following figure illustrates the architecture and the interaction
   between the nodes.
  </para>

  <figure xml:id="fig.deploy.architecture.cluster">
   <title>&productname; Architecture</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="caas_architecture.svg" width="100%"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="caas_architecture.png" width="100%"/>
    </imageobject>
    <textobject role="description">
     <phrase>&productname; Architecture</phrase>
    </textobject>
   </mediaobject>
  </figure>

  <sect2 xml:id="sec.deploy.architecture.administration-node">
   <title>The &Admin_Node;</title>
   <para>
    The &admin_node; manages the cluster and runs several applications required
    for proper functioning of the cluster. Because it is integral to the
    operation of &productname;, the &admin_node; must have a fully-qualified
    domain name (FQDN), for example <systemitem
    class="fqdomainname">admin.caas.&exampledomain;</systemitem>, which can be
    resolved from outside the cluster.
   </para>
   <para>
    The &admin_node; runs the following services in separate containers (refer
    to <xref linkend="sec.deploy.components"/> for details):
   </para>
   <itemizedlist>
    <listitem>
     <para>
      the &dashboard;, the administration dashboard
     </para>
    </listitem>
    <listitem>
     <para>
      the MariaDB database
     </para>
    </listitem>
    <listitem>
     <para>
      the <systemitem class="daemon">etcd</systemitem> discovery daemon
     </para>
    </listitem>
    <listitem>
     <para>
      the &salt; components <systemitem class="daemon">salt-api</systemitem>,
      <systemitem class="daemon">salt-master</systemitem>, <systemitem
      class="daemon">salt-minion</systemitem>
     </para>
    </listitem>
   </itemizedlist>
   <para>
    The containers on the &admin_node; are managed by <literal>kubelet</literal>
    as a static pod. Note that this <literal>kubelet</literal> does not
    manage the cluster nodes. Each cluster node has its own running instance of
    <literal>kubelet</literal>.
   </para>
  </sect2>

  <sect2 xml:id="sec.deploy.architecture.master-nodes">
   <title>The &Master_Node;(s)</title>
   <para>
    &productname; &master_node;s monitor and control the &worker_node;s. They
    make global decisions about the cluster, such as starting and scheduling
    pods of containers on the &worker_node;s. They run
    <literal>kube-apiserver</literal> but do not host application containers.
   </para>
   <para>
    Each cluster must have at least one &master_node;. For larger clusters,
    more &master_node;s can be added, but there must always be an odd number.
   </para>
   <para>
    Like the &admin_node;, the &master_node; must have a resolvable FQDN, for
    example <systemitem
    class="fqdomainname">master.caas.&exampledomain;</systemitem>. For
    &dashboard; to function correctly, it must always be able to resolve the IP
    address of a &master_node;, so if there are multiple &master_node;s, they
    must all share the same FQDN, meaning that load-balancing should be
    configured.
    <remark condition="clarity">
     2018-09-18 - fs: How to set this up? This at least requires some pointer
    </remark>
   </para>
  </sect2>
  <sect2 xml:id="sec.deploy.architecture.worker_node">
   <title>&Worker_Node;s</title>
   <para>
    The &worker_node;s are the machines in the cluster which host application
    containers. Each runs its own instance of <literal>kubelet</literal>
    which controls the pods on that machine. Earlier versions of &kube;
    referred to &worker_node;s as "minions".
   </para>
   <para>
    Each &worker_node; runs a container runtime engine (either
    <literal>&docker;</literal> or <literal>&crio;</literal>) and an instance
    of <literal>kube-proxy</literal> (refer to <xref
    linkend="sec.deploy.components"/> for details).
   </para>
   <para>
    The &worker_node;s do not require individual FQDNs, although it may help in
    troubleshooting network problems.
   </para>
  </sect2>
  <sect2 xml:id="sec.deploy.architecture.smt">
   <title>&smtool; Server (&smt;)</title>
   <para>
    The &smt; server, shipping with &sls; 12, makes &suse; software
    subscriptions available on site. It regularly synchronizes repository data
    from &scc;, making all software packages, updates and patches
    available within your network.
   </para>
   <para>
    In case your organization already runs an &smt; server, it can also be used
    for &productname;. All nodes need to be able to access the &smt; server and
    the server needs to be configured to mirror the &productname; repositories.
   </para>
   <para>
    Information on how to set up and operate an &smt; server is available at
    <link
      xlink:href="https://www.suse.com/documentation/sles-12/book_smt/data/book_smt.html"/>.
   </para>

   <remark condition="clarity">
    2018-09-19 - fs: Section on Log Host is missing
   </remark>

  </sect2>
<!-- 2018-09-19 - fs: Missing, requires input
  <sect2 xml:id="sec.deploy.architecture.log">
   <title>Log Host</title>
   <para/>
  </sect2>
-->
 </sect1>

 <sect1 xml:id="sec.deploy.components">
  <title>Software Components and Technologies</title>
  <para>
   &productname; lets you choose between two different container
   runtime engines, &docker; and &crio;. &docker; is used by default. &crio;
   has been added with &productname; &productnumber; and is currently available
   as technology preview.
  </para>
  <variablelist>
   <title>Container Runtime Engines</title>
   <varlistentry>
    <term>&docker;</term>
    <listitem>
     <para>
      This is the leading open-source format for application containers. It is
      fully supported by &suse;. For more information, see <link
      xlink:href="https://www.docker.com/"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>&crio;</term>
    <listitem>
     <para>
      As a technology preview, &productname; includes the &crio; container
      runtime in addition to &docker;. &crio; is an implementation of CRI, the
      Container Runtime Interface and was designed specifically for
      Kubernetes. The development focus of &crio; is creating a lightweight and
      architecturally simple container engine that is also stable and
      secure. Like &docker;, it supports Open Container Initiative (OCI)
      images.
     </para>
     <para>
       During the installation, you can choose whether to set up the complete
       cluster with &docker;, the default, or with the &crio;. As both use the
       same runtime component (<literal>runC</literal>), the usage of a
       container engine is transparent, no changes to workloads and images are
       needed.
     </para>
     <para>
      For more information about &crio;, see <link
      xlink:href="https://cri-o.io/"/>.
     </para>
     <important>
      <title>Container Engine Support Status</title>
      <para>
       &crio; is included as an <emphasis>unsupported technology
       preview</emphasis>, to allow customers to evaluate the new technology.
       It is <emphasis>not</emphasis> supported for use in production
       deployments.
      </para>
     </important>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   To deploy and manage nodes and to manage and set up cluster networking, the
   following technologies are used:
  </para>

  <variablelist>
   <title>Node Management and Networking Components</title>
   <varlistentry>
    <term>&salt;</term>
    <listitem>
     <para>
      &salt; is used to manage deployment and administration of the cluster.
      <literal>&salt;-api</literal> is used to distribute commands from
      &dashboard; to the <systemitem class="daemon">salt-master</systemitem>
      daemon. It stores events in a MariaDB database, which is also used to
      store &dashboard; data. The <systemitem
      class="daemon">salt-minion</systemitem> daemon on the &admin_node;
      generates the required certificates, and <systemitem
      class="daemon">salt-minion</systemitem> daemons on the &worker_node;s
      communicate with the &admin_node;.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>&dashboard;</term>
    <listitem>
     <para>
      &dashboard; is a Web application that enables you to deploy, manage, and
      monitor the cluster. The dashboard manages the cluster using <systemitem
      class="daemon">salt-api</systemitem> to interact with the underlying
      &salt; technology.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>etcd</term>
    <listitem>
     <para>
      Within the cluster there are several instances of etcd, each with a
      different purpose. The <systemitem class="daemon">etcd</systemitem>
      discovery daemon running on the &admin_node; is used to bootstrap
      instances of etcd on other nodes and is not part of the
      <literal>etcd</literal> cluster on the other nodes. The etcd instance on
      the &master_node; stores events from the &kube; API Server. The etcd
      instance on &worker_node;s runs as a proxy that forwards clients to the
      <systemitem class="daemon">etcd</systemitem> on the &master_node;.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Flannel</term>
    <listitem>
     <para>
      As there can be multiple containers running on each host machine, each
      container is assigned an IP address that is used for communication with
      other containers on the same host. Containers might need to have a unique
      IP address exposed for network communications, thus Flannel networking is
      used. Flannel gives each host an IP subnet from which the container
      engine can allocate IP addresses to containers. The mapping of IP
      addresses is stored by etcd. The <systemitem
      class="daemon">flanneld</systemitem> daemon manages routing of packets
      and mapping of IP addresses.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   Container orchestration is managed by &kube;. The following services
   and daemons are running on the master and worker nodes:
  </para>

  <variablelist>
   <title>Container Orchestration Components</title>
   <varlistentry>
    <term>
     <literal>kubelet</literal>
    </term>
    <listitem>
     <para>
      An agent that runs on each node to monitor and control all the containers
      in a pod, ensuring that they are running and healthy.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     <literal>kube-apiserver</literal>
    </term>
    <listitem>
     <para>
      This daemon exposes a REST API used to manage pods. The API server
      performs authentication and authorization.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     <literal>kube-scheduler</literal>
    </term>
    <listitem>
     <para>
      The scheduler assigns pods to &worker_node;s. It does not run them
      itself; that is <literal>kubelet</literal>'s job.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     <literal>controllers</literal>
    </term>
    <listitem>
     <para>
      Controllers monitor the shared state of the cluster through the
      <literal>apiserver</literal> and handle pod replication, deployment, etc.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     <literal>kube-proxy</literal>
    </term>
    <listitem>
     <para>
      <remark condition="clarity">
       2018-09-18 - fs: Is the following really true?
      </remark>
      This runs on each node and is used to distribute loads and reach services.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <figure xml:id="fig.deploy.architecture.services">
   <title>Services on &productname;</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="caasp_nodes_architecture.svg" width="100%"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="caasp_nodes_architecture.svg" width="100%"/>
    </imageobject>
    <textobject role="description">
     <phrase>Services on nodes</phrase>
    </textobject>
   </mediaobject>
  </figure>
 </sect1>

</chapter>
