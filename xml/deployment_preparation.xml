<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter version="5.0" xml:id="cha.deployment.preparation"
 xmlns="http://docbook.org/ns/docbook"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink">
 <info>
  <title>Preparing the Installation</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker/>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>
 <para>
  Depending on the environment, &productname; can be installed using several
  methods. Each method requires differnt steps for preparation:
 </para>
 <variablelist>
  <!-- Installing on Bare Metal or Virtual Machines -->
  <varlistentry>
   <term><xref linkend="sec.deploy.preparation.baremetal"/></term>
   <listitem>
    <para>
     An installtion on physical hardware or virtual machines gives you the
     choice of three different installation methods:
    </para>
    <itemizedlist>
     <listitem>
      <!-- Installing from USB, DVD or ISO Images -->
      <para>
       <xref linkend="sec.deploy.preparation.dvd" xrefstyle="select:title"/>:
       This method requires manual interaction with the physical or virtual
       machine. This method is suitable for small to medium cluster sizes and
       can be used for manual and &ay; installations.
      </para>
     </listitem>
     <listitem>
      <!-- Installing from Network Source -->
      <para>
       <xref linkend="sec.deploy.preparation.pxe" xrefstyle="select:title"/>:
       With this method the boot image is provided by a network installation
       server. The nodes boot the image using network boot (PXE). This method
       reduces the need to interact with every single node. It is suitable for
       medium cluster sizes and can be used for manual and &ay; installations.
      </para>
     </listitem>
     <listitem>
      <!-- Installing from Virtual Disk Images -->
      <para>
       <xref linkend="sec.deploy.preparation.disk_images"
       xrefstyle="select:title"/>: You can use prepared disk images ready to
       deploy on virtual machines. Since node configuration is done
       automatically via <command>cloud-init</command>, this increases the
       deployment speed.  This method is suitable for medium to large cluster
       sizes.  It cannot be used for &ay; installations. This method is also
       recommended when installing on VMware ESX and ESXi environments, since
       the images alrteady contain the <package>open-vm-tools</package>, which
       otherwise need to be installed separately on each node.
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
  </varlistentry>
  <varlistentry>
   <term>
    <!-- Installing in SUSE OpenStack Cloud -->
    <xref linkend="sec.deploy.preparation.openstack" xrefstyle="select:title"/>
   </term>
   <listitem>
    <para>
     &suse; provides disk images and &ostack; Heat teamplates required for
     deploying &productname; on &soc;.
    </para>
   </listitem>
  </varlistentry>
  <varlistentry>
   <!-- Installing in Public Cloud -->
   <term>
    <xref linkend="sec.deploy.preparation.public_cloud"
          xrefstyle="select:title"/>
   </term>
   <listitem>
    <para>
     This section is about installing &productname; in a public cloud,
     for example <emphasis>Microsoft Azure</emphasis>*,
     <emphasis>Amazon AWS</emphasis>* and <emphasis>Google Compute
     Engine</emphasis>*.
    </para>
   </listitem>
  </varlistentry>
 </variablelist>


 <sect1 xml:id="sec.deploy.preparation.baremetal">
  <title>Installing on Bare Metal or Virtual Machines</title>
  <para>
   An installtion on physical hardware or virtual machines can be done by
   booting from an ISO image or a network source. Virtual machines can also be
   installed from virtual disk images.
  </para>

  <sect2 xml:id="sec.deploy.preparation.dvd">
   <title>Installing from USB, DVD or ISO Images</title>
   <para>
   This procedure provides an overview of the steps for the cluster deployment
   with classical boot devices like DVD drives. This method is suitable for
   installing small to medium clusters on bare metal or virtual machines.
   </para>
   <procedure>
    <step>
     <para>
     Choose an installation medium. You can install from DVD or a USB
     flash medium. On virtual machines, you can install from ISO images.
    </para>
    <tip>
     <title>Creating a Bootable USB Flash Drive</title>
     <para>
      To create a bootable flash disk, you need to copy a DVD image to the
      device using the dd command. The flash disk must not be mounted, all data
      on the device will be erased:
     </para>
     <screen>&prompt.sudo;dd if=<replaceable>PATH_TO_ISO_IMAGE</replaceable> of=<replaceable>USB_STORAGE_DEVICE</replaceable> bs=4M</screen>
    </tip>
   </step>
   <step>
    <para>
     Boot the machine designated to become the &admin_node; from the
     selected medium. Follow the installation instructions at <xref
     linkend="sec.deploy.nodes.admin_install"/>.
    </para>
   </step>
   <step>
    <para>
     Configure the &admin_node; as described in <xref
     linkend="sec.deploy.nodes.admin_configuration"/> until you see the
     <guimenu>Select nodes and roles</guimenu> screen.
    </para>
   </step>
   <step>
    <para>
     Boot the machines designated to become the &master_node;(s) and the
     &worker_node;s from the selected medium. The same ISO image as for booting
     and installing the &admin_node; is also used for the other node types. To
     set up the nodes you have two choices:
    </para>
    <itemizedlist>
    <listitem>
     <para>
      Setting up the nodes manually. You will have to enter data such as
      language, keyboard, registration code and the &admin_node;'s address on
      single setup dialog.  This dialog is very similar to the one that was
      used to set up the &admin_node;. See <xref
      linkend="sec.deploy.nodes.worker_install"/> for details. Only use this
      method for small cluster sizes (up to five nodes).
     </para>
    </listitem>
    <listitem>
     <para>
      Setting up the nodes automatically with &ay;. The &admin_node; serves a
      ready-to-use &ay; file which can be downloaded by the nodes during
      boot. This considerably speeds up the node deployment and is therefore
      the recommended method. See <xref
      linkend="sec.deploy.nodes.worker_install.manual.autoyast"/> for details.
     </para>
    </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Finish the installation by bootstrapping the nodes. For details,
     see <xref linkend="sec.deploy.install.bootstrap"/>.
    </para>
   </step>
   <step>
    <para>
     For deployments on &vmware; ESX/ESXi: install the
     <package>open-vm-tools</package> package. See <xref
     linkend="sec.deploy.install.vmware_tools" />.
    </para>
   </step>
  </procedure>
 </sect2>

 <sect2 xml:id="sec.deploy.preparation.pxe">
  <title>Installing from a Network Source</title>
  <para>
   This procedure provides an overview of the steps for the cluster deployment
   from an network installation server. A PXE environment is used to provide
   the nodes with the data required for installation. This method is suitable
   for installing small to medium clusters on bare metal or virtual machines.
  </para>
  <procedure>
   <step>
    <para>
     Provide an installation server that provides a DHCP, PXE and TFTP
     service. Additionally, you can provide the installation data on
     an HTTP or FTP server. Such a server can be set up using &sle;,
     installation instructions are available at:
    </para>
    <itemizedlist>
     <listitem>
      <para><link
       xlink:href="https://www.suse.com/documentation/sles-12/singlehtml/book_sle_deployment/book_sle_deployment.html#cha.deployment.instserver">&slea; 12 SP3: Setting Up an Installation Server</link>
      </para>
     </listitem>
     <listitem>
      <para>
       <link
       xlink:href="https://www.suse.com/documentation/sles-15/singlehtml/book_sle_deployment/book_sle_deployment.html#part.installserver">&slea;
       15; Setting Up an Installation Server</link>
      </para>
     </listitem>
    </itemizedlist>
    <substeps>
     <step>
      <para>
       To provide the Kernel and initrd, you can mount the ISO image of the
       installation media and use the files
       <filename>/boot/x86_64/loader/linux</filename> and
       <filename>/boot/x86_64/loader/initrd</filename> from this medium.
      </para>
      <para>
       Alternatively, install the package
       <package>tftpboot-installation-CAASP-3.0</package> on the installation
       server. This package can also be found on the installation media. It is
       located at
       <filename>/suse/noarch/tftpboot-installation-CAASP-3.0-x86_64-<replaceable>VERSION</replaceable>.noarch.rpm</filename>. The
       package provides the Kernel and initrd in the /srv/tftpboot/
       directory. To install it, mount the ISO image of the installation media
       und use zypper for installing, for example:
      </para>
      <screen>&prompt.sudo;mount -o loop SUSE-CaaS-Platform-3.0-DVD-x86_64-GM-DVD1.iso /mnt
&prompt.sudo;zypper in \
      /mnt/suse/noarch/tftpboot-installation-CAASP-3.0-x86_64-14.337.3-1.7.noarch.rpm</screen>
     </step>
     <step>
      <para> The examples in the server installation instructions refer to
      &sle;&mdash;make sure to adjust the paths to correctly point to the files
      for &productname;.
      </para>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     PXE boot the machine designated to become the &admin_node; from the
     selected medium. Follow the installation instructions at <xref
     linkend="sec.deploy.nodes.admin_install"/>.
    </para>
   </step>
   <step>
    <para>
     Configure the &admin_node; as described in <xref
     linkend="sec.deploy.nodes.admin_configuration"/> until you see the
     <guimenu>Select nodes and roles</guimenu> screen.
    </para>
   </step>
   <step>
    <para>
     PXE boot the machines designated to become the &master_node;(s) and the
     &worker_node;s from the selected medium. The same ISO image as for booting
     and installing the &admin_node; is also used for the other node types. To
     set up the nodes you have two choices:
    </para>
    <itemizedlist>
    <listitem>
     <para>
      Setting up the nodes manually. You will have to enter data such as
      language, keyboard, registration code and the &admin_node;'s address on
      single setup dialog.  This dialog is very similar to the one that was
      used to set up the &admin_node;. See <xref
      linkend="sec.deploy.nodes.worker_install"/> for details. Only use this
      method for small cluster sizes (up to five nodes).
     </para>
    </listitem>
    <listitem>
     <para>
      Setting up the nodes automatically with &ay;. The &admin_node; serves a
      ready-to-use &ay; file which can be downloaded by the nodes during
      boot. This considerably speeds up the node deployment and is therefore
      the recommended method. See <xref
      linkend="sec.deploy.nodes.worker_install.manual.autoyast"/> for details.
     </para>
    </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Finish the installation by bootstrapping the nodes. For details,
     see <xref linkend="sec.deploy.install.bootstrap"/>.
    </para>
   </step>
   <step>
    <para>
     For deployments on &vmware; ESX/ESXi: install the
     <package>open-vm-tools</package> package. See <xref
     linkend="sec.deploy.install.vmware_tools" />.
    </para>
   </step>
  </procedure>
 </sect2>

 <sect2 xml:id="sec.deploy.preparation.disk_images">
  <title>Installing from Virtual Disk Images</title>
  <para>
   For building clusters from virtual machines on supported
   hypervisors, it is not necessary to individually install each
   node. &suse; offers pre-installed VM disk images for the following
   hypervisors:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     <emphasis role="bold">&kvm; and &xen; (fully virtualized)</emphasis>: In
     QCOW2 format
     (<filename>SUSE-CaaS-Platform-3.0-for-KVM-and-Xen.x86_64-3.0.0-GM.qcow2</filename>)
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold">&xen; (paravirtualized)</emphasis>: In QCOW2 format
     (<filename>SUSE-CaaS-Platform-3.0-for-XEN.x86_64-3.0.0-GM.qcow2</filename>)
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold">VMware ESX/ESXi</emphasis>: In VMDK format
     (<filename>SUSE-CaaS-Platform-3.0-for-VMware.x86_64-3.0.0-GM.vmdk</filename>)
    </para>
    <para>
     Before you can use the image, it needs to be converted. On the ESX/ESXi
     host, run the following command:
    </para>
    <screen>&prompt.root;<command>vmkfstools \
-i <replaceable>DOWNLOAD_IMAGE</replaceable>.vmdk \ <replaceable>CONVERTED_IMAGE</replaceable>.vmdk</command></screen>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold">Microsoft Hyper-V</emphasis>: In VHD format
     (<filename>SUSE-CaaS-Platform-3.0-for-MS-HyperV.x86_64-3.0.0-GM.vhdfixed.xz</filename>)
    </para>
   </listitem>
  </itemizedlist>

  <para>
   When deploying a cluster node from pre-installed disk images, the
   setup program never runs. Therefore, must happen
   while the node is starting up. For this purpose, &productname;
   includes <command>cloud-init</command> to apply setup data to each instance.
   Therefore it is necessary to provide the respective data for cloud-init as
   an ISO-image for each node that gets launched. This image gets attached as a
   virtual drive to each virtual machine, so that cloud-init cann acess the
   data, when the machine is launched by booting the disk image.
  </para>

  <procedure>
   <title>Overview: Installing from Virtual Disk Images</title>
   <step>
    <para>
     Download the disk images and create a copy of the downloaded disk image
     for each virtual machine.
    </para>
   </step>
   <step>
    <para>
     Write <command>cloud-init</command> configuration files and Create ISO
     images containing these configuration files for each virtual machine. See
     <xref linkend="sec.deploy.preparation.disk_images.configuration"/>.
    </para>
   </step>
   <step>
    <para>
     Configure and launch the virtual machines. See <xref
     linkend="sec.deploy.preparation.qcow2.nodes" />.
    </para>
   </step>
  </procedure>

  <sect3 xml:id="sec.deploy.preparation.disk_images.configuration">
   <title>Creating the cloud-init Configuration</title>
   <para>
    A &productname; virtual disk image is used to set up all three node types
    (&admin_node;, &worker_node; &master_node;). To define the node type and to
    provide additional configuration data, <command>cloud-init</command> is
    used to apply this data to each instance.
   </para>
   <para>
    <command>cloud-init</command> uses two yaml-formatted configuration files
    to provide the data for each node:
   </para>
   <variablelist>
    <varlistentry>
     <term><filename>meta-data</filename></term>
     <listitem>
      <para>
       In public clouds, the metadata is usually provided by the cloud
       vendor. With &productname; it is used to provide a unique instance-ID
       and the network interface setup for each instance.
      </para>
      <para>
       Because the instance-ID needs to be unique, a different
       <filename>meta-data</filename> is required for each node.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>user-data</filename></term>
     <listitem>
      <para>
       The data provided with this file is used to set up and configure the
       different node types.
      </para>
      <para>
       Because &master_node;s and &worker_node;s can be set up with the same
       configuration, only two different <filename>user-data</filename>, one
       for the &admin_node; and one for all other nodes are required. If
       required by your setup, you may alternatively provide individual
       <filename>user-data</filename> files for each node.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>

   <para>
    The following examples show <command>cloud-init</command> configuration
    files for &productname; containing the minimum of required data. This
    should be sufficient for most setups. More information describing
    additional configuration options for <command>cloud-init</command> is
    available at <xref linkend="sec.deploy.cloud-init"/>.
   </para>

   <example xml:id="ex.ci.meta-data">
    <title><filename>meta-data</filename></title>

    <screen>#cloud-config <co xml:id="co.meta.start"/>
# Meta Data File for &productname;
instance-id: <replaceable>admin</replaceable> <co xml:id="co.meta.id"/>
network-interfaces: | <co xml:id="co.meta.net"/>
   auto <replaceable>eth0</replaceable> <co xml:id="co.meta.net-start"/>
   iface <replaceable>eth0</replaceable> inet dhcp <co xml:id="co.meta.net-config"/></screen>

    <calloutlist>
     <callout arearefs="co.meta.start">
      <para>
       Each configuration file needs to start with this line.
       <remark condition="clarity">
        2018-10-05 - fs: Is this true???
       </remark>
      </para>
     </callout>
     <callout arearefs="co.meta.id">
      <para>
       The instance ID. Needs to be unique for each node. See <xref
       linkend="ex.ci.geniso"/> for an example on how to automatically create
       such a unique ID.
      </para>
     </callout>
     <callout arearefs="co.meta.net">
      <para>
       Network configuration section.
      </para>
     </callout>
     <callout arearefs="co.meta.net-start">
      <para>
       Define when to start a network interface. In this example, the interface
       <replaceable>eth0</replaceable> is started during boot.
      </para>
     </callout>
     <callout arearefs="co.meta.net-config">
      <para>
       Network interface configuration. In this example, the interface
       <replaceable>eth0</replaceable> is configured via DHCP. In case you
       require a static configuration, use the following values:
      </para>
      <screen>iface eth0 inet static
  address <replaceable>NODE_ADDRESS</replaceable>
  network <replaceable>NETWORk_ADDRESS</replaceable>
  netmask <replaceable>NETMASK</replaceable>
  broadcast <replaceable>BROADCAST_ADDRESS</replaceable>
  gateway <replaceable>GATEWAY_ADDRESS</replaceable></screen>
     </callout>
    </calloutlist>
   </example>

   <example xml:id="ex.ci.user-data">
    <title><filename>user-data</filename></title>

    <informaltable>
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="50*"/>
      <colspec colnum="2" colname="2" colwidth="50*"/>
      <thead>
       <row>
        <entry><para>Admin Node</para></entry>
        <entry><para>Master/Worker Nodes</para></entry>
       </row>
      </thead>
      <tbody>
       <row>
        <entry>
         <screen>#cloud-config <co xml:id="co.user.start"/>
# Admin Node
# User Data File
debug: True <co xml:id="co.user.debug"/>
disable_root: False <co xml:id="co.user.login"/>
ssh_deletekeys: False
ssh_pwauth: True
chpasswd: <co xml:id="co.user.root_pw"/>
   list: |
     root:<replaceable>ROOT_PASSWORD</replaceable>
     expire: False
ntp: <co xml:id="co.user.ntp"/>
   servers:
     - <replaceable>ntp1.&exampledomain;</replaceable>
     - <replaceable>ntp2.&exampledomain;</replaceable>
runcmd: <co xml:id="co.user.runcmd"/>
   - /usr/bin/systemctl enable --now ntpd
suse_caasp: <co xml:id="co.user.caas"/>
  role: admin</screen>
        </entry>
        <entry valign="top">
         <screen>#cloud-config <xref linkend="co.user.start" xrefstyle="select:label nopage"/>
# Master/Worker Node
# User Data File
debug: True <xref linkend="co.user.debug" xrefstyle="select:label nopage"/>
disable_root: False <xref linkend="co.user.login" xrefstyle="select:label nopage"/>
ssh_deletekeys: False
ssh_pwauth: True
chpasswd: <xref linkend="co.user.root_pw" xrefstyle="select:label nopage"/>
   list: |
   root:<replaceable>ROOT_PASSWORD</replaceable>
     expire: False
suse_caasp: <xref linkend="co.user.caas" xrefstyle="select:label nopage"/>
    role: cluster
    admin_node: <replaceable>caas-admin.example.com</replaceable></screen>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>

    <calloutlist>
     <callout arearefs="co.user.start">
      <para>
       Each configuration file needs to start with this line.
       <remark condition="clarity">
        2018-10-05 - fs: Is this true???
       </remark>
      </para>
     </callout>
     <callout arearefs="co.user.debug">
      <para>
       Turns on debugging output. Useful in case errors occur.
      </para>
     </callout>
     <callout arearefs="co.user.login">
      <para>
       Three options that define how to log in to the node.
      </para>
      <simplelist>
       <member>
        <literal>disable_root: False</literal> disables passwordless
        authentication for &rootuser;
       </member>
       <member>
        <literal>ssh_deletekeys: False</literal> disables automatic deletion of
        existing SSH host keys. This setting is strongly recommended, because
        otherwise the host keys will change each time the cloud-init
        configuration is changed.
        <remark condition="clarity">
         2018-10-05 - fs: Is that true??
        </remark>
       </member>
       <member>
        <literal>ssh_pwauth: True</literal> allows to log in with a password
        via SSH
       </member>
      </simplelist>
     </callout>
     <callout arearefs="co.user.root_pw">
      <para>
       Set the &rootuser; password for the node. It can be specified as plain
       text or as a hash generated by <command>mkpasswd</command>. See <xref
       linkend="sec.deploy.cloud-init.user-data.password"/> for details.
      </para>
     </callout>
     <callout arearefs="co.user.ntp">
      <para>
       Specify a list of NTP servers here. This setting is only used for the
       &admin_node;.
      </para>
     </callout>
     <callout arearefs="co.user.runcmd">
      <para>
       Specify a list of commands to be executed. Enabling the <systemitem
       class="daemon">ntpd</systemitem> as in this example is required on the
       &admin_node;.
      </para>
     </callout>
     <callout arearefs="co.user.caas">
      <para>
       Set the node type here. For the &admin_node; it is <literal>role:
       admin</literal>, for all other nodes <literal>role: cluster</literal>.
       The value for <literal>admin_node:</literal> (FQDN of the &admin_node;)
       only needs to be specified for &worker_node;s and &master_node;s. For
       these node types it is mandatory.
      </para>
     </callout>
    </calloutlist>
   </example>

   <para>
    Create pairs of <filename>meta-data</filename> and
    <filename>user-data</filename> files as described above for each node you
    like to deploy. Then create an ISO image containing these files for each
    node using the command <command>mkisofs</command>.
   </para>

   <example xml:id="ex.ci.geniso">
    <title>Creating cloud-init ISO Images for all Nodes</title>
    <para>
     The following example shows how to create ISO images for all nodes. Images
     for the &master_node;s and &worker_node;s are created from a template, so
     you only need to create two sets of configuration files.
    </para>
    <procedure>
     <step>
      <para>
       Change to a clean working directory.
      </para>
     </step>
     <step>
      <para>
       Create a text file <filename>cluster_nodes.txt</filename> with node
       names for all &master_node;s and &worker_node;s. The names need to be
       unique and each line must only contain one name, for example:
      </para>
      <screen>worker1
worker2
worker3
master</screen>
     </step>
     <step>
      <para>
       In the working directory, create a <filename>meta-data</filename>
       file. Specify <literal>§§§§§</literal> as value for
       <literal>instance-id</literal>. It will later automatically be replaced
       with the node name.
      </para>
      <screen>instance-id: §§§§§</screen>
      <para>
       In the working directory, also create a <filename>user-data</filename>
       file suitable for &master_node;s and &worker_node;s.
      </para>
     </step>
     <step>
      <para>
       Run the following commands to create ISO images for all nodes listed in
       <filename>cluster_nodes.txt</filename>:
      </para>
      <screen>
for NODE in $(cat cluster_nodes.txt); do
  mkdir $NODE
  cp user-data $NODE
  sed -e "s/§§§§§/${NODE}/" &lt; meta-data > $NODE/meta-data
  mkisofs -output ${NODE}.iso -volid ${NODE}-data -joliet -rock $NODE
  rm -rf $NODE
done</screen>
     </step>
     <step>
      <para>
       Create a directory for the &admin_node;, for example
       <literal>admin/</literal>. Create the files
       <filename>admin/meta-data</filename> and
       <filename>admin/user-data</filename> containing the configuration for
       the &admin_node;. Specify a unique value for
       <literal>instance-id</literal>, for example <literal>admin</literal>:
      </para>
      <screen>instance-id: admin</screen>
     </step>
     <step>
      <para>
       Create the ISO image for the &admin_node;:
      </para>
      <screen>mkisofs -output admin.iso -volid admin-data -joliet -rock admin</screen>
     </step>
     <step>
      <para>
       The working directory now contains ISO images for all nodes specified in
       <filename>cluster_nodes.txt</filename> plus one for the &admin_node;.
      </para>
     </step>
    </procedure>
   </example>
  </sect3>

  <sect3 xml:id="sec.deploy.preparation.qcow2.nodes">
   <title>Launching the Nodes</title>
   <para>
    To launch the nodes you need a copy of the virtual disk image for each node
    and the node-specific ISO images created in <xref
    linkend="sec.deploy.preparation.disk_images.configuration"/>.
   </para>
   <procedure>
    <step>
     <para>
      Create a new virtual machine for the &admin_node;.
     </para>
    </step>
    <step>
     <para>
      Attach a copy of the disk image as its main hard disk.
     </para>
    </step>
    <step>
     <para>
      Attach the ISO image for the &admin_node;
      (<filename>admin.iso</filename>) as a virtual CDROM.
    </para>
    </step>
    <step>
     <para>
      Start the virtual machine.
     </para>
    </step>
    <step>
     <para>
      Configure the &admin_node; as described in <xref
      linkend="sec.deploy.nodes.admin_configuration"/> until you see the
      <guimenu>Select nodes and roles</guimenu> screen.
     </para>
    </step>
    <step>
     <para>
      For each &master_node; and &worker_node;, do the following:
     </para>
     <substeps>
      <step>
       <para>
        Create a new virtual machine for the node.
       </para>
      </step>
      <step>
       <para>
        Attach a copy of the disk image as its main hard disk.
       </para>
      </step>
      <step>
       <para>
        Attach the ISO image for this specific node as a virtual CDROM.
       </para>
      </step>
      <step>
       <para>
        Start the virtual machine.
       </para>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      Finish the installation by bootstrapping the nodes. For details,
      see <xref linkend="sec.deploy.install.bootstrap"/>.
     </para>
    </step>
   </procedure>
  </sect3>
 </sect2>
</sect1>



 <sect1 xml:id="sec.deploy.preparation.openstack">
  <title>Installing in &soc;</title>
  <para>
   You can deploy a &productname; on &soc; using &ostack;.
   You will need a &productname; machine image and &ostack; Heat templates.
   Once you have created a stack, you will continue with the &productname; setup.
  </para>
  <note>
   <title>&productname; machine image for &soc;</title>
   <para>
    Download the latest &productname; for &ostack; image from
    <link xlink:href="https://download.suse.com">https://download.suse.com</link>
    (for example,
    <filename>SUSE-CaaS-Platform-3.0-OpenStack-Cloud.x86_64-1.0.0-GM.qcow2</filename>).
   </para>
  </note>
   <note>
    <title>
     &ostack; Heat Templates Repository
    </title>
    <para>
     &productname; Heat templates are available from <link xlink:href="https://github.com/SUSE/caasp-openstack-heat-templates">GitHub</link>.
    </para>
  </note>

  <sect2 xml:id="sec.deploy.preparation.openstack.horizon">
   <title>Using the Horizon Dashboard</title>
   <procedure>
   <step>
    <para>
     Go to <guimenu>Project &rarr; Compute &rarr; Images</guimenu> and click on
     <guimenu>Create Image</guimenu>.
    </para>
    <para>
     Give your image a name (for example: <literal>CaaSP-3</literal>); you will
     need to use this later to find the image.
    </para>
   </step>
    <step>
     <para>
      Go to <guimenu>Project &rarr; Orchestration</guimenu> and click on
      <guimenu>Stacks</guimenu>.
     </para>
     <informalfigure>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="horizon_stacks.png" width="100%"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="horizon_stacks.png" width="100%"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
    <step>
     <para>
      Click on <guimenu>Launch Stack</guimenu> and provide the stack
      templates. Either upload the files, provide the URL to the raw files
      directly (only applies to stack template), or copy and paste the
      contents into the <guimenu>Direct Input</guimenu> fields.
     </para>
     <warning>
      <title>Replace the default <literal>root_password</literal></title>
       <para>
        Do not use the <filename>caasp-environment.yaml</filename> directly
        from the GitHub repository.
       </para>
        <para>
         You must make sure to replace the value for
         <literal>root_password</literal> with a secure password.
         This will become the password for the &rootuser; account on all
         nodes in the stack.
        </para>
    </warning>
     <informalfigure>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="horizon_launch_stack.png" width="100%"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="horizon_launch_stack.png" width="100%"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
   <step>
    <para>
     Click <guimenu>Next</guimenu>.
    </para>
   </step>
    <step>
     <para>
      Now you need to define more information about the stack.
     </para>
     <variablelist>
      <varlistentry>
       <term><literal>Stack Name</literal></term>
       <listitem>
        <para>
         Give your stack a name
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><literal>Password</literal></term>
       <listitem>
        <para>
         Your &soc; password
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><literal>Image</literal></term>
       <listitem>
        <para>
         Select the image your machines will be created from
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><literal>root_password</literal></term>
       <listitem>
        <para>
         Set the root password for your cluster machines
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><literal>admin</literal>/master/worker_flavor</term>
       <listitem>
        <para>
         Select the machine flavor for your nodes
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><literal>worker_count</literal></term>
       <listitem>
        <para>
         Number of worker nodes to be launched
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><literal>external_net</literal></term>
       <listitem>
        <para>
         Select an external network that your cluster will be reachable from
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><literal>internal_net_cidr</literal></term>
       <listitem>
        <para>
         The internal network range to be used inside the cluster
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><literal>dns_nameserver</literal></term>
       <listitem>
        <para>
         Internal name server for the cluster
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
     <informalfigure>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="horizon_stack_options.png" width="100%"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="horizon_stack_options.png" width="100%"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
    <step>
     <para>
     Click <guimenu>Launch</guimenu>.
     </para>
    </step>
    <step>
     <para>
      After the cluster has been started and the cluster overview shows
      <guimenu>Create Complete</guimenu>, you need to find the external IP
      address for the admin node of your cluster (here:
      <literal>10.86.1.72</literal>). Now visit that IP address in your browser.
      You should see the &dashboard; login page and can continue with
      <xref linkend="sec.deploy.nodes.admin_configuration"/>.
     </para>
     <informalfigure>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="horizon_stack_resources.png" width="100%"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="horizon_stack_resources.png" width="100%"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec.deploy.preparation.openstack.cli">
   <title>Using the &ostack; CLI</title>
   <note>
    <para>
     You need to have access to the &ostack; command-line tools. You can
     either access those via <literal>ssh</literal> on your &soc; admin
     server or
     <link xlink:href="https://docs.openstack.org/newton/user-guide/common/cli-install-openstack-command-line-clients.html">
     install a local <command>openstack</command> client</link>.
    </para>
    <para>
     To use the local client, you need to access <guimenu>Project &rarr; Compute &rarr;
     Access &amp; Security</guimenu> in the Horizon Dashboard and click on the
     <guimenu>Download &ostack; RC File v3</guimenu>.
    </para>
    <para>
     The downloaded file is a script that you then need to load using the
     <command>source</command> command. The script will ask you for your
     &soc; password.
    </para>
     <screen>&prompt.user;<command>source container-openrc.sh</command></screen>
   </note>
   <procedure>
     <step>
      <para>
       Upload the container image to &ostack; Glance (Image service). This
       example uses the name <literal>CaaSP-3</literal> as the name of the
       image that is created in &soc;.
      </para>
     <screen>&prompt.user;<command>openstack image create --public --disk-format qcow2 \
--container-format bare \
--file SUSE-CaaS-Platform-3.0-OpenStack-Cloud.x86_64-1.0.0-GM.qcow2 \
<replaceable>CaaSP-3</replaceable></command></screen>
     </step>
     <step>
      <warning>
       <title>Replace the default <literal>root_password</literal></title>
        <para>
         Do not use the <filename>caasp-environment.yaml</filename> directly
         from the GitHub repository.
        </para>
        <para>
         You must make sure to replace the value for
         <literal>root_password</literal> with a secure password.
         This will become the password for the &rootuser; account on all
         nodes in the stack.
        </para>
     </warning>
      <para>
       Download the <filename>caasp-stack.yaml</filename> and
       <filename>caasp-environment.yaml</filename> Heat templates to your
       workstation and then run the <command>openstack stack create</command>
       command.
      </para>
      <screen>&prompt.user;<command>openstack stack create \
-t caasp-stack.yaml \
-e caasp-environment.yaml \
--parameter image=<replaceable>CaaSP-3</replaceable> <replaceable>caasp3-stack</replaceable></command></screen>
     </step>

    <step>
     <para>
      Find out which (external) IP address was assigned to the admin node
      of your &productname; cluster (here: <literal>10.81.1.51</literal>).
     </para>
     <screen>&prompt.user;<command>openstack server list --name "admin" | awk 'FNR > 3 {print $4 $5 $9}'</command>
caasp3-stack-admin|10.81.1.51</screen>
     </step>
     <step>
      <para>
       Visit the external IP address in your browser. You should see the &dashboard;
       login page and can continue with <xref linkend="sec.deploy.nodes.admin_configuration"/>.
      </para>
    </step>
   </procedure>
  </sect2>
 </sect1>

 <sect1 xml:id="sec.deploy.preparation.public_cloud">
  <title>Installing in Public Cloud</title>
  <sect2 xml:id="sec.deploy.preparation.public_cloud.overview">
   <title>Overview</title>
   <para>
    The &productname; images published by &suse; in selected Public Cloud
    environments are provided as <literal>Bring Your Own Subscription
    (BYOS)</literal> images. &productname; instances need to be registered with
    the &scc; in order to receive bugfix and security
    updates. Images labeled with the <literal>cluster</literal> designation in
    the name are not intended to be started directly; they are deployed by the
    Administrative node. Administrative node images contain the
    <literal>admin</literal> designation in the image name.
   </para>
   <para>
    The following procedure outlines the deployment process:
   </para>
   <procedure>
    <step>
     <para>
      Read the special system requirements for public cloud
      installations in <xref
      linkend="sec.deploy.requirements.environment.public_cloud"/>.
     </para>
    </step>
    <step>
     <para>
      Provision the cluster nodes. For details, see <xref linkend=
      "sec.deploy.preparation.public_cloud.provisioning" />.
     </para>
    </step>
    <step>
     <para>
      Deploy the admin node with <command>caasp-admin-setup</command>.
      For details, see <xref
      linkend="sec.deploy.nodes.admin_install_cli" />.
     </para>
    </step>
    <step>
     <para>
      Finish bootstrapping your cluster. The provisioned worker nodes
      are ready to be consumed into the cluster. For details, see <xref
      linkend= "sec.deploy.install.bootstrap" />.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec.deploy.preparation.public_cloud.provisioning">
   <title>Provisioning Cluster Nodes</title>
   <sect3 xml:id="sec.deploy.preparation.public_cloud.provisioning.aws">
    <title>Amazon Web Services EC2</title>
    <para>
     You may select from one of the predefined instance types, hand selected
     for general container workloads, or choose <guimenu>Other
     types...</guimenu> and enter any <guimenu>instance type</guimenu>, as
     defined at <link xlink:href="https://aws.amazon.com/ec2/instance-types/">
     https://aws.amazon.com/ec2/instance-types/</link>
    </para>
    <para>
     Two configuration options are required in EC2:
    </para>
    <variablelist>
     <varlistentry>
      <term>Subnet ID</term>
      <listitem>
       <para>
        The <literal>subnet</literal> within which cluster nodes will be
        attached to the network, in the form
        <literal>subnet-xxxxxxxx</literal>.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Security Group ID</term>
      <listitem>
       <para>
        The <literal>security group</literal> defining network access rules for
        the cluster nodes, in the form <literal>sg-xxxxxxxx</literal>.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <para>
     The defaults used for those two options are preset to the subnet ID of the
     administration host and the security group ID that was automatically
     created by <command>caasp-admin-setup</command>. You may choose to place
     the cluster nodes in a different subnet and you can also use a custom
     security group, but please bear in mind that traffic must be allowed
     between the individual cluster nodes and also between the admininstration
     node and the cluster nodes.
    </para>
    <para>
     See the <link xlink:href="https://aws.amazon.com/documentation/vpc/">
     Amazon Virtual Private Cloud Documentation</link>for more information.
    </para>
   </sect3>
   <sect3 xml:id="sec.deploy.preparation.public_cloud.provisioning.azure">
    <title>Microsoft Azure</title>
    <para>
     You need to configure credentials for access to the Azure framework so
     instances can be created, as well as parameters for the cluster node
     instances themselves. The credentials refer to authentication via a
     service principal. See
     <link xlink:href="https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-group-create-service-principal-portal">
     https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-group-create-service-principal-portal</link>
     for more information on how you can create a service principal.
    </para>
    <variablelist>
     <varlistentry>
      <term>Subscription ID</term>
      <listitem>
       <para>
        The subscription ID of your Azure account.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Tenant ID</term>
      <listitem>
       <para>
        The tenant ID of your service principal, also known as the Active
        Directory ID.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Application ID</term>
      <listitem>
       <para>
        The application ID of your service principal.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Client Secret</term>
      <listitem>
       <para>
        The key value or password of your service principal.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <para>
     Below the <guimenu>Service Principal Authentication</guimenu> box you will
     find the <guimenu>Instance Type</guimenu> configuration. You may select
     from one of the predefined instance types, hand selected for general
     container workloads, or choose <guimenu>Other types...</guimenu> and enter
     any <literal>size</literal>, as defined at
     <link xlink:href="https://docs.microsoft.com/en-us/azure/virtual-machines/linux/sizes/">
     https://docs.microsoft.com/en-us/azure/virtual-machines/linux/sizes/</link>Set
     the <guimenu>Cluster size</guimenu> using the slider.
    </para>
    <para>
     The parameters in <guimenu>Resource Scopes</guimenu> define attributes of
     the cluster instances, as required for Azure Resource Manager:
    </para>
    <variablelist>
     <varlistentry>
      <term>Resource Group</term>
      <listitem>
       <para>
        The Resource Group in which all cluster nodes will be created.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Storage Account</term>
      <listitem>
       <para>
        The Storage Account that will be used for storing the cluster node OS
        disks. See
        <link xlink:href="https://docs.microsoft.com/en-us/azure/storage/common/storage-create-storage-account">
        https://docs.microsoft.com/en-us/azure/storage/common/storage-create-storage-account</link>for
        more information about Azure Storage Accounts.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Network</term>
      <listitem>
       <para>
        The virtual network the cluster nodes will be connected to.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Subnet</term>
      <listitem>
       <para>
        A subnet in the previously defined virtual network. See
        <link xlink:href="https://docs.microsoft.com/en-us/azure/virtual-network/">
        https://docs.microsoft.com/en-us/azure/virtual-network/</link> for more
        information about Azure Virtual Networks.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
   <sect3 xml:id="sec.deploy.preparation.public_cloud.provisioning.google">
    <title>Google Compute Engine</title>
    <para>
     You may select from one of the predefined instance types, hand selected
     for general container workloads, or choose <guimenu>Other
     types...</guimenu> and enter any <literal>machine type</literal>, as
     defined at
     <link xlink:href="https://cloud.google.com/compute/docs/machine-types">
     https://cloud.google.com/compute/docs/machine-types</link>
    </para>
    <para>
     Two configuration options are required in GCE:
    </para>
    <variablelist>
     <varlistentry>
      <term>Network</term>
      <listitem>
       <para>
        The name of the virtual network the cluster nodes will run within.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Subnet</term>
      <listitem>
       <para>
        If you created a custom network, you must specify the name of the
        subnet within which the cluster nodes will run.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <para>
     See the <link xlink:href="https://cloud.google.com/vpc/docs/vpc"> GCE
     Network Documentation</link> for more information.
    </para>
   </sect3>
  </sect2>
 </sect1>
</chapter>
