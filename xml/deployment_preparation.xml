<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter version="5.0" xml:id="cha.deployment.preparation"
 xmlns="http://docbook.org/ns/docbook"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink">
 <info>
  <title>Preparing the Installation</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker/>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>
 <para>
  Depending on the environment, &productname; can be installed using several
  methods. Each method requires different steps for preparation:
 </para>
 <variablelist>
  <!-- Installing on Bare Metal or Virtual Machines -->
  <varlistentry>
   <term><xref linkend="sec.deploy.preparation.baremetal"/></term>
   <listitem>
    <para>
     An installation on physical hardware or virtual machines gives you the
     choice of three different installation methods:
    </para>
    <itemizedlist>
     <listitem>
      <!-- Installing from Flash Disks, DVD or ISO Images -->
      <para>
       <xref linkend="sec.deploy.preparation.dvd" xrefstyle="select:title"/>:
       This method requires manual interaction with the physical or virtual
       machine. This method is suitable for small to medium cluster sizes and
       can be used for manual and &ay; installations.
      </para>
     </listitem>
     <listitem>
      <!-- Installing from Network Source -->
      <para>
       <xref linkend="sec.deploy.preparation.pxe" xrefstyle="select:title"/>:
       With this method the boot image is provided by a network installation
       server. The nodes boot the image using network boot (PXE). This method
       reduces the need to interact with every single node. It is suitable for
       medium cluster sizes and can be used for manual and &ay; installations.
      </para>
     </listitem>
     <listitem>
      <!-- Installing from Virtual Disk Images -->
      <para>
       <xref linkend="sec.deploy.preparation.disk_images"
       xrefstyle="select:title"/>: You can use prepared disk images ready to
       deploy on virtual machines. Since node configuration is done
       automatically via <command>cloud-init</command>, this increases the
       deployment speed.  This method is suitable for medium to large cluster
       sizes.  It cannot be used for &ay; installations. This method is also
       recommended when installing on VMware ESX and ESXi environments, since
       the images already contain the <package>open-vm-tools</package>, which
       otherwise need to be installed separately on each node.
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
  </varlistentry>
  <varlistentry>
   <term>
    <!-- Installing in SUSE OpenStack Cloud -->
    <xref linkend="sec.deploy.preparation.openstack" xrefstyle="select:title"/>
   </term>
   <listitem>
    <para>
     &suse; provides disk images and &ostack; Heat templates required for
     deploying &productname; on &soc;.
    </para>
   </listitem>
  </varlistentry>
  <varlistentry>
   <!-- Installing in Public Cloud -->
   <term>
    <xref linkend="sec.deploy.preparation.public_cloud"
          xrefstyle="select:title"/>
   </term>
   <listitem>
    <para>
     This section is about installing &productname; in a public cloud,
     for example <emphasis>Microsoft Azure</emphasis>*,
     <emphasis>Amazon AWS</emphasis>* and <emphasis>Google Compute
     Engine</emphasis>*.
    </para>
   </listitem>
  </varlistentry>
 </variablelist>


 <sect1 xml:id="sec.deploy.preparation.baremetal">
  <title>Preparing a Bare Metal / Virtual Machine Installation</title>
  <para>
   An installation on physical hardware or virtual machines can be done by
   booting from an ISO image or a network source. Virtual machines can also be
   installed from virtual disk images.
  </para>

  <sect2 xml:id="sec.deploy.preparation.dvd">
   <title>Installing from Flash Disks, DVD or ISO Images</title>
   <para>
   This procedure provides an overview of the steps for the cluster deployment
   with classical boot devices like DVD drives. This method is suitable for
   installing small to medium clusters on bare metal or virtual machines.
   </para>
   <procedure>
    <step>
     <para>
     Choose an installation medium. You can install from DVD or a flash
     disk medium. On virtual machines, you can install from ISO images.
    </para>
    <tip>
     <title>Creating a Bootable Flash Disk</title>
     <para>
      To create a bootable flash disk, you need to copy a DVD image to the
      device using the dd command. The flash disk must not be mounted, all data
      on the device will be erased:
     </para>
     <screen>&prompt.sudo;dd if=<replaceable>PATH_TO_ISO_IMAGE</replaceable> of=<replaceable>USB_STORAGE_DEVICE</replaceable> bs=4M</screen>
    </tip>
   </step>
   <step>
    <para>
     Boot the machine designated to become the &admin_node; from the
     selected medium. Follow the installation instructions at <xref
     linkend="sec.deploy.nodes.admin_install"/>.
    </para>
   </step>
   <step>
    <para>
     Configure the &admin_node; as described in <xref
     linkend="sec.deploy.nodes.admin_configuration"/> until you see the
     <guimenu>Select nodes and roles</guimenu> screen.
    </para>
   </step>
   <step>
    <para>
     Boot the machines designated to become the &master_node;(s) and the
     &worker_node;s from the selected medium. The same ISO image as for booting
     and installing the &admin_node; is also used for the other node types. To
     set up the nodes you have two choices:
    </para>
    <itemizedlist>
    <listitem>
     <para>
      Setting up the nodes manually. You will have to enter data such as
      language, keyboard, registration code and the &admin_node;'s address on
      single setup dialog.  This dialog is very similar to the one that was
      used to set up the &admin_node;. See <xref
      linkend="sec.deploy.nodes.worker_install"/> for details. Only use this
      method for small cluster sizes (up to five nodes).
     </para>
    </listitem>
    <listitem>
     <para>
      Setting up the nodes automatically with &ay;. The &admin_node; serves a
      ready-to-use &ay; file which can be downloaded by the nodes during
      boot. This considerably speeds up the node deployment and is therefore
      the recommended method. See <xref
      linkend="sec.deploy.nodes.worker_install.manual.autoyast"/> for details.
     </para>
    </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Finish the installation by bootstrapping the nodes. For details,
     see <xref linkend="sec.deploy.install.bootstrap"/>.
    </para>
   </step>
   <step>
    <para>
     For deployments on &vmware; ESX/ESXi: install the
     <package>open-vm-tools</package> package. See <xref
     linkend="sec.deploy.install.vmware_tools" />.
    </para>
   </step>
  </procedure>
 </sect2>

 <sect2 xml:id="sec.deploy.preparation.pxe">
  <title>Installing from a Network Source</title>
  <para>
   This procedure provides an overview of the steps for the cluster deployment
   from an network installation server. A PXE environment is used to provide
   the nodes with the data required for installation. This method is suitable
   for installing small to medium clusters on bare metal or virtual machines.
  </para>
  <procedure>
   <step>
    <para>
     Provide an installation server that provides a DHCP, PXE and TFTP
     service. Additionally, you can provide the installation data on
     an HTTP or FTP server. Such a server can be set up using &sle;,
     installation instructions are available at:
    </para>
    <itemizedlist>
     <listitem>
      <para><link
       xlink:href="https://www.suse.com/documentation/sles-12/singlehtml/book_sle_deployment/book_sle_deployment.html#cha.deployment.instserver">&slea; 12 SP3: Setting Up an Installation Server</link>
      </para>
     </listitem>
     <listitem>
      <para>
       <link
       xlink:href="https://www.suse.com/documentation/sles-15/singlehtml/book_sle_deployment/book_sle_deployment.html#part.installserver">&slea;
       15; Setting Up an Installation Server</link>
      </para>
     </listitem>
    </itemizedlist>
    <substeps>
     <step>
      <para>
       To provide the Kernel and initrd, you can mount the ISO image of the
       installation media and use the files
       <filename>/boot/x86_64/loader/linux</filename> and
       <filename>/boot/x86_64/loader/initrd</filename> from this medium.
      </para>
      <para>
       Alternatively, install the package
       <package>tftpboot-installation-CAASP-3.0</package> on the installation
       server. This package can also be found on the installation media. It is
       located at
       <filename>/suse/noarch/tftpboot-installation-CAASP-3.0-x86_64-<replaceable>VERSION</replaceable>.noarch.rpm</filename>. The
       package provides the Kernel and initrd in the /srv/tftpboot/
       directory. To install it, mount the ISO image of the installation media
       and use zypper for installing, for example:
      </para>
      <screen>&prompt.sudo;mount -o loop SUSE-CaaS-Platform-3.0-DVD-x86_64-GM-DVD1.iso /mnt
&prompt.sudo;zypper in \
      /mnt/suse/noarch/tftpboot-installation-CAASP-3.0-x86_64-14.337.3-1.7.noarch.rpm</screen>
     </step>
     <step>
      <para> The examples in the server installation instructions refer to
      &sle;&mdash;make sure to adjust the paths to correctly point to the files
      for &productname;.
      </para>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     PXE boot the machine designated to become the &admin_node; from the
     selected medium. Follow the installation instructions at <xref
     linkend="sec.deploy.nodes.admin_install"/>.
    </para>
   </step>
   <step>
    <para>
     Configure the &admin_node; as described in <xref
     linkend="sec.deploy.nodes.admin_configuration"/> until you see the
     <guimenu>Select nodes and roles</guimenu> screen.
    </para>
   </step>
   <step>
    <para>
     PXE boot the machines designated to become the &master_node;(s) and the
     &worker_node;s from the selected medium. The same ISO image as for booting
     and installing the &admin_node; is also used for the other node types. To
     set up the nodes you have two choices:
    </para>
    <itemizedlist>
    <listitem>
     <para>
      Setting up the nodes manually. You will have to enter data such as
      language, keyboard, registration code and the &admin_node;'s address on
      single setup dialog.  This dialog is very similar to the one that was
      used to set up the &admin_node;. See <xref
      linkend="sec.deploy.nodes.worker_install"/> for details. Only use this
      method for small cluster sizes (up to five nodes).
     </para>
    </listitem>
    <listitem>
     <para>
      Setting up the nodes automatically with &ay;. The &admin_node; serves a
      ready-to-use &ay; file which can be downloaded by the nodes during
      boot. This considerably speeds up the node deployment and is therefore
      the recommended method. See <xref
      linkend="sec.deploy.nodes.worker_install.manual.autoyast"/> for details.
     </para>
    </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Finish the installation by bootstrapping the nodes. For details,
     see <xref linkend="sec.deploy.install.bootstrap"/>.
    </para>
   </step>
   <step>
    <para>
     For deployments on &vmware; ESX/ESXi: install the
     <package>open-vm-tools</package> package. See <xref
     linkend="sec.deploy.install.vmware_tools" />.
    </para>
   </step>
  </procedure>
 </sect2>

 <sect2 xml:id="sec.deploy.preparation.disk_images">
  <title>Installing from Virtual Disk Images</title>
  <para>
   For building clusters from virtual machines on supported
   hypervisors, it is not necessary to individually install each
   node. &suse; offers pre-installed VM disk images on <link
   xlink:href="https://download.suse.com/"/> for the following
   hypervisors:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     <emphasis role="bold">&kvm; and &xen; (fully virtualized)</emphasis>: In
     QCOW2 format
     (<filename>SUSE-CaaS-Platform-3.0-KVM-and-Xen.x86_64-1.0.0-GM.qcow2</filename>)
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold">&xen; (paravirtualized)</emphasis>: In QCOW2 format
     (<filename>SUSE-CaaS-Platform-3.0-XEN.x86_64-1.0.0-GM.qcow2</filename>)
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold">VMware ESX/ESXi</emphasis>: In VMDK format
     (<filename>SUSE-CaaS-Platform-3.0-VMware.x86_64-1.0.0-GM.vmdk</filename>)
    </para>
    <para>
     Before you can use the image, it needs to be converted. On the ESX/ESXi
     host, run the following command:
    </para>
    <screen>&prompt.root;<command>vmkfstools-i <replaceable>DOWNLOAD_IMAGE</replaceable>.vmdk <replaceable>CONVERTED_IMAGE</replaceable>.vmdk</command></screen>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold">Microsoft Hyper-V</emphasis>: In VHD format
     (<filename>SUSE-CaaS-Platform-3.0-MS-HyperV.x86_64-1.0.0-GM.vhdfixed.xz</filename>)
    </para>
   </listitem>
  </itemizedlist>

  <para>
   When deploying a cluster node from pre-installed disk images, the
   setup program never runs. Therefore, must happen
   while the node is starting up. For this purpose, &productname;
   includes <command>cloud-init</command> to apply setup data to each instance.
   Therefore it is necessary to provide the respective data for cloud-init as
   an ISO-image for each node that gets launched. This image gets attached as a
   virtual drive to each virtual machine, so that cloud-init can access the
   data, when the machine is launched by booting the disk image.
  </para>

  <procedure>
   <title>Overview: Installing from Virtual Disk Images</title>
   <step>
    <para>
     Download the disk images and create a copy of the downloaded disk image
     for each virtual machine.
    </para>
   </step>
   <step>
    <para>
     Write <command>cloud-init</command> configuration files and Create ISO
     images containing these configuration files for each virtual machine. See
     <xref linkend="sec.deploy.preparation.disk_images.configuration"/>.
    </para>
   </step>
   <step>
    <para>
     Configure and launch the virtual machines. See <xref
     linkend="sec.deploy.preparation.qcow2.nodes" />.
    </para>
   </step>
  </procedure>

  <sect3 xml:id="sec.deploy.preparation.disk_images.configuration">
   <title>Creating the cloud-init Configuration</title>
   <para>
    A &productname; virtual disk image is used to set up all three node types
    (&admin_node;, &worker_node; &master_node;). To define the node type and to
    provide additional configuration data, <command>cloud-init</command> is
    used to apply this data to each instance.
   </para>
   <para>
    <command>cloud-init</command> uses two yaml-formatted configuration files
    to provide the data for each node:
   </para>
   <variablelist>
    <varlistentry>
     <term><filename>meta-data</filename></term>
     <listitem>
      <para>
       In public clouds, the metadata is usually provided by the cloud
       vendor. With &productname; it is used to provide a unique instance-ID
       and the network interface setup for each instance.
      </para>
      <para>
       Because the instance-ID needs to be unique, a different
       <filename>meta-data</filename> is required for each node.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>user-data</filename></term>
     <listitem>
      <para>
       The data provided with this file is used to set up and configure the
       different node types.
      </para>
      <para>
       Because &master_node;s and &worker_node;s can be set up with the same
       configuration, only two different <filename>user-data</filename>, one
       for the &admin_node; and one for all other nodes are required. If
       required by your setup, you may alternatively provide individual
       <filename>user-data</filename> files for each node.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>

   <para>
    The following examples show <command>cloud-init</command> configuration
    files for &productname; containing the minimum of required data. This
    should be sufficient for most setups. More information describing
    additional configuration options for <command>cloud-init</command> is
    available at <xref linkend="sec.deploy.cloud-init"/>.
   </para>

   <example xml:id="ex.ci.meta-data">
    <title><filename>meta-data</filename></title>

    <screen>#cloud-config <co xml:id="co.meta.start"/>
# Meta Data File for &productname;
instance-id: <replaceable>admin</replaceable> <co xml:id="co.meta.id"/>
network-interfaces: | <co xml:id="co.meta.net"/>
   auto <replaceable>eth0</replaceable> <co xml:id="co.meta.net-start"/>
   iface <replaceable>eth0</replaceable> inet dhcp <co xml:id="co.meta.net-config"/></screen>

    <calloutlist>
     <callout arearefs="co.meta.start">
      <para>
       Each configuration file needs to start with this line.
       <remark condition="clarity">
        2018-10-05 - fs: Is this true???
       </remark>
      </para>
     </callout>
     <callout arearefs="co.meta.id">
      <para>
       The instance ID. Needs to be unique for each node. See <xref
       linkend="ex.ci.geniso"/> for an example on how to automatically create
       such a unique ID.
      </para>
     </callout>
     <callout arearefs="co.meta.net">
      <para>
       Network configuration section.
      </para>
     </callout>
     <callout arearefs="co.meta.net-start">
      <para>
       Define when to start a network interface. In this example, the interface
       <replaceable>eth0</replaceable> is started during boot.
      </para>
     </callout>
     <callout arearefs="co.meta.net-config">
      <para>
       Network interface configuration. In this example, the interface
       <replaceable>eth0</replaceable> is configured via DHCP. In case you
       require a static configuration, use the following values:
      </para>
      <screen>iface eth0 inet static
  address <replaceable>NODE_ADDRESS</replaceable>
  network <replaceable>NETWORk_ADDRESS</replaceable>
  netmask <replaceable>NETMASK</replaceable>
  broadcast <replaceable>BROADCAST_ADDRESS</replaceable>
  gateway <replaceable>GATEWAY_ADDRESS</replaceable></screen>
     </callout>
    </calloutlist>
   </example>

   <example xml:id="ex.ci.user-data">
    <title><filename>user-data</filename></title>

    <informaltable>
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="50*"/>
      <colspec colnum="2" colname="2" colwidth="50*"/>
      <thead>
       <row>
        <entry><para>Admin Node</para></entry>
        <entry><para>Master/Worker Nodes</para></entry>
       </row>
      </thead>
      <tbody>
       <row>
        <entry>
         <screen>#cloud-config <co xml:id="co.user.start"/>
# Admin Node
# User Data File
debug: True <co xml:id="co.user.debug"/>
disable_root: False <co xml:id="co.user.login"/>
ssh_deletekeys: False
ssh_pwauth: True
chpasswd: <co xml:id="co.user.root_pw"/>
   list: |
     root:<replaceable>ROOT_PASSWORD</replaceable>
     expire: False
ntp: <co xml:id="co.user.ntp"/>
   servers:
     - <replaceable>ntp1.&exampledomain;</replaceable>
     - <replaceable>ntp2.&exampledomain;</replaceable>
runcmd: <co xml:id="co.user.runcmd"/>
   - /usr/bin/systemctl enable --now ntpd
suse_caasp: <co xml:id="co.user.caas"/>
  role: admin</screen>
        </entry>
        <entry valign="top">
         <screen>#cloud-config <xref linkend="co.user.start" xrefstyle="select:label nopage"/>
# Master/Worker Node
# User Data File
debug: True <xref linkend="co.user.debug" xrefstyle="select:label nopage"/>
disable_root: False <xref linkend="co.user.login" xrefstyle="select:label nopage"/>
ssh_deletekeys: False
ssh_pwauth: True
chpasswd: <xref linkend="co.user.root_pw" xrefstyle="select:label nopage"/>
   list: |
   root:<replaceable>ROOT_PASSWORD</replaceable>
     expire: False
suse_caasp: <xref linkend="co.user.caas" xrefstyle="select:label nopage"/>
    role: cluster
    admin_node: <replaceable>caas-admin.example.com</replaceable></screen>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>

    <calloutlist>
     <callout arearefs="co.user.start">
      <para>
       Each configuration file needs to start with this line.
       <remark condition="clarity">
        2018-10-05 - fs: Is this true???
       </remark>
      </para>
     </callout>
     <callout arearefs="co.user.debug">
      <para>
       Turns on debugging output. Useful in case errors occur.
      </para>
     </callout>
     <callout arearefs="co.user.login">
      <para>
       Three options that define how to log in to the node.
      </para>
      <simplelist>
       <member>
        <literal>disable_root: False</literal> disables passwordless
        authentication for &rootuser;
       </member>
       <member>
        <literal>ssh_deletekeys: False</literal> disables automatic deletion of
        existing SSH host keys. This setting is strongly recommended, because
        otherwise the host keys will change each time the cloud-init
        configuration is changed.
        <remark condition="clarity">
         2018-10-05 - fs: Is that true??
        </remark>
       </member>
       <member>
        <literal>ssh_pwauth: True</literal> allows to log in with a password
        via SSH
       </member>
      </simplelist>
     </callout>
     <callout arearefs="co.user.root_pw">
      <para>
       Set the &rootuser; password for the node. It can be specified as plain
       text or as a hash generated by <command>mkpasswd</command>. See <xref
       linkend="sec.deploy.cloud-init.user-data.password"/> for details.
      </para>
     </callout>
     <callout arearefs="co.user.ntp">
      <para>
       Specify a list of NTP servers here. This setting is only used for the
       &admin_node;.
      </para>
     </callout>
     <callout arearefs="co.user.runcmd">
      <para>
       Specify a list of commands to be executed. Enabling the <systemitem
       class="daemon">ntpd</systemitem> as in this example is required on the
       &admin_node;.
      </para>
     </callout>
     <callout arearefs="co.user.caas">
      <para>
       Set the node type here. For the &admin_node; it is <literal>role:
       admin</literal>, for all other nodes <literal>role: cluster</literal>.
       The value for <literal>admin_node:</literal> (FQDN of the &admin_node;)
       only needs to be specified for &worker_node;s and &master_node;s. For
       these node types it is mandatory.
      </para>
     </callout>
    </calloutlist>
   </example>

   <para>
    Create pairs of <filename>meta-data</filename> and
    <filename>user-data</filename> files as described above for each node you
    like to deploy. Then create an ISO image containing these files for each
    node using the command <command>mkisofs</command>.
   </para>

   <example xml:id="ex.ci.geniso">
    <title>Creating cloud-init ISO Images for all Nodes</title>
    <para>
     The following example shows how to create ISO images for all nodes. Images
     for the &master_node;s and &worker_node;s are created from a template, so
     you only need to create two sets of configuration files.
    </para>
    <procedure>
     <step>
      <para>
       Change to a clean working directory.
      </para>
     </step>
     <step>
      <para>
       Create a text file <filename>cluster_nodes.txt</filename> with node
       names for all &master_node;s and &worker_node;s. The names need to be
       unique and each line must only contain one name, for example:
      </para>
      <screen>worker1
worker2
worker3
master</screen>
     </step>
     <step>
      <para>
       In the working directory, create a <filename>meta-data</filename>
       file. Specify <literal>§§§§§</literal> as value for
       <literal>instance-id</literal>. It will later automatically be replaced
       with the node name.
      </para>
      <screen>instance-id: §§§§§</screen>
      <para>
       In the working directory, also create a <filename>user-data</filename>
       file suitable for &master_node;s and &worker_node;s.
      </para>
     </step>
     <step>
      <para>
       Run the following commands to create ISO images for all nodes listed in
       <filename>cluster_nodes.txt</filename>:
      </para>
      <screen>
for NODE in $(cat cluster_nodes.txt); do
  mkdir $NODE
  cp user-data $NODE
  sed -e "s/§§§§§/${NODE}/" &lt; meta-data > $NODE/meta-data
  mkisofs -output ${NODE}.iso -volid ${NODE}-data -joliet -rock $NODE
  rm -rf $NODE
done</screen>
     </step>
     <step>
      <para>
       Create a directory for the &admin_node;, for example
       <literal>admin/</literal>. Create the files
       <filename>admin/meta-data</filename> and
       <filename>admin/user-data</filename> containing the configuration for
       the &admin_node;. Specify a unique value for
       <literal>instance-id</literal>, for example <literal>admin</literal>:
      </para>
      <screen>instance-id: admin</screen>
     </step>
     <step>
      <para>
       Create the ISO image for the &admin_node;:
      </para>
      <screen>mkisofs -output admin.iso -volid admin-data -joliet -rock admin</screen>
     </step>
     <step>
      <para>
       The working directory now contains ISO images for all nodes specified in
       <filename>cluster_nodes.txt</filename> plus one for the &admin_node;.
      </para>
     </step>
    </procedure>
   </example>
  </sect3>

  <sect3 xml:id="sec.deploy.preparation.qcow2.nodes">
   <title>Launching the Nodes</title>
   <para>
    To launch the nodes you need a copy of the virtual disk image for each node
    and the node-specific ISO images created in <xref
    linkend="sec.deploy.preparation.disk_images.configuration"/>.
   </para>
   <procedure>
    <step>
     <para>
      Create a new virtual machine for the &admin_node;.
     </para>
    </step>
    <step>
     <para>
      Attach a copy of the disk image as its main hard disk.
     </para>
    </step>
    <step>
     <para>
      Attach the ISO image for the &admin_node;
      (<filename>admin.iso</filename>) as a virtual CDROM.
    </para>
    </step>
    <step>
     <para>
      Start the virtual machine.
     </para>
    </step>
    <step>
     <para>
      Configure the &admin_node; as described in <xref
      linkend="sec.deploy.nodes.admin_configuration"/> until you see the
      <guimenu>Select nodes and roles</guimenu> screen.
     </para>
    </step>
    <step>
     <para>
      For each &master_node; and &worker_node;, do the following:
     </para>
     <substeps>
      <step>
       <para>
        Create a new virtual machine for the node.
       </para>
      </step>
      <step>
       <para>
        Attach a copy of the disk image as its main hard disk.
       </para>
      </step>
      <step>
       <para>
        Attach the ISO image for this specific node as a virtual CDROM.
       </para>
      </step>
      <step>
       <para>
        Start the virtual machine.
       </para>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      Finish the installation by bootstrapping the nodes. For details,
      see <xref linkend="sec.deploy.install.bootstrap"/>.
     </para>
    </step>
   </procedure>
  </sect3>
 </sect2>
</sect1>

<sect1 xml:id="sec.deploy.preparation.openstack">
  <title>Preparing a &soc; Installation</title>
  <para>
   You can deploy a &productname; on &soc; using &ostack;. You will need a
   &productname; machine image and &ostack; Heat templates which are provided
   by &suse;.
  </para>

  <sect2 xml:id="sec.deploy.preparation.openstack.prep">
   <title>Preparation</title>
   <para>
    Upload the &productname; image to the &soc; and make the eat templates
    available:
   </para>
   <procedure>
    <step>
     <para>
      Download the latest &productname; for &ostack; image from <link
      xlink:href="https://download.suse.com"/> (for example,
      <filename>SUSE-CaaS-Platform-3.0-OpenStack-Cloud.x86_64-1.0.0-GM.qcow2</filename>).
     </para>
    </step>
    <step>
     <para>
      Upload the image to Glance:
     </para>
     <screen>&prompt.user;openstack image create --public --disk-format qcow2 \
--container-format bare --file <replaceable>IMAGE</replaceable> CaaSP-3</screen>
     <para>
      In this example, the image is named <literal>CaaSP-3</literal>&mdash;you
      will need this name later in the process.
     </para>
    </step>
    <step>
     <para>
      The &ostack; Heat templates are named
      <filename>caasp-environment.yaml</filename> and
      <filename>caasp-stack.yaml</filename>.
     </para>
     <para>
      On &soc; 8 and later, they can be made available by installing the
      package <package>caasp-openstack-heat-templates</package>.
     </para>
     <screen>&prompt.sudo;zypper in caasp-openstack-heat-templates</screen>
     <para>
      The template files will be installed to
      <filename>/usr/share/caasp-openstack-heat-templates</filename>.
     </para>
     <para>
      For &soc; 7, download the two template files from <link
      xlink:href="https://github.com/SUSE/caasp-openstack-heat-templates">GitHub</link>..
     </para>
     <screen>&prompt.user;wget https://raw.githubusercontent.com/SUSE/caasp-openstack-heat-templates/master/caasp-environment.yaml
&prompt.user;wget  https://raw.githubusercontent.com/SUSE/caasp-openstack-heat-templates/master/caasp-stack.yaml</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec.deploy.preparation.openstack.horizon">
   <title>Launching the Stack Using the Horizon Dashboard</title>
   <procedure>
    <step>
     <para>
      Log in to the &soc; the Horizon Dashboard.
     </para>
    </step>
    <step>
     <para>
      Go to <menuchoice><guimenu>Project</guimenu>
      <guimenu>Orchestration</guimenu> <guimenu>Stacks</guimenu></menuchoice>.
     </para>
     <informalfigure>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="horizon_stacks.png" width="100%"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="horizon_stacks.png" width="100%"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
    <step>
     <para>
      Click on <guimenu>Launch Stack</guimenu> to open the <guimenu>Select
      Template</guimenu> dialog.
     </para>
     <informalfigure>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="horizon_launch_stack.png" width="100%"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="horizon_launch_stack.png" width="100%"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
    <step>
     <para>
      Under <guimenu>Template Source</guimenu> provide the
      <filename>caasp-stack.yaml</filename> file. Under <guimenu>Environment
      Source</guimenu> provide the <filename>caasp-environment.yaml</filename>.
     </para>
     <para>
      You may either upload the respective files or copy and paste the
      contents into the <guimenu>Direct Input</guimenu> fields. The
      <guimenu>Template Source</guimenu> can also be provided as a URL to the
      raw file.
     </para>
    </step>
   <step>
    <para>
     Click <guimenu>Next</guimenu> to proceed to the <guimenu>Launch
     Stack</guimenu> dialog.
    </para>
    <informalfigure>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="horizon_stack_options.png" width="100%"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="horizon_stack_options.png" width="100%"/>
      </imageobject>
     </mediaobject>
    </informalfigure>
   </step>
   <step>
    <para>
     Now you need to provide additional information on the stack. You may also
     adjust the data provided by the environment template:
    </para>
    <variablelist>
     <title>Stack Meta Data</title>
     <varlistentry>
      <term><guimenu>Stack Name</guimenu></term>
      <listitem>
       <para>
        Provide a Name for the stack.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><guimenu>Creation Timeout</guimenu></term>
      <listitem>
       <para>
        Provide the number of minutes before the for the creation of the stack
        times out. The more nodes you launch, the larger the timeout should be.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><guimenu>Rollback on Failure</guimenu></term>
      <listitem>
       <para>
        Performs a rollback in case the stack creation fails or times out.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><guimenu>Password for user "User"</guimenu></term>
       <listitem>
        <para>
         Your &soc; password.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>Image</guimenu></term>
       <listitem>
        <para>
         Select the &productname; image you uploaded earlier. It will be listed
         by the name you provided on upload (<literal>CaaSP-3</literal> in the
         example above). The nodes will be launched from that image.
        </para>
       </listitem>
      </varlistentry>
    </variablelist>

    <para>
     The following data was provided via the
     <filename>caasp-environment.yaml</filename> template file. You should at
     least adjust the &rootuser; password and the number of worker nodes that
     should be launched. Also make sure the networking data is correct.
    </para>

    <variablelist>
     <title>Environment Data</title>
     <varlistentry>
      <term><guimenu>root_password</guimenu></term>
      <listitem>
       <para>
        Set the &rootuser; password that will be used for all nodes.
       </para>
       <warning>
        <title>Provide a Secure &rootuser; Password</title>
        <para>
         The &rootuser; password is set to <literal>linux</literal> by
         default. It is strongly recommended to replace it with a secure
         password.
        </para>
       </warning>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><guimenu>admin_flavor</guimenu></term>
      <listitem>
       <para>
        Optionally, change the machine flavor for the admin node.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><guimenu>master_flavor</guimenu></term>
      <listitem>
       <para>
        Optionally, change the machine flavor for the master node.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><guimenu>worker_flavor</guimenu></term>
      <listitem>
       <para>
        Optionally, change the machine flavor for the worker nodes.
       </para>
      </listitem>
     </varlistentry>
      <varlistentry>
       <term><guimenu>worker_count</guimenu></term>
       <listitem>
        <para>
         Number of worker nodes to be launched
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>external_net</guimenu></term>
       <listitem>
        <para>
         The external network from which your cluster can be reached.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>internal_net_cidr</guimenu></term>
       <listitem>
        <para>
         The internal network address to be used inside the cluster.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><guimenu>dns_nameserver</guimenu></term>
       <listitem>
        <para>
         Internal name server for the cluster.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
    </step>
    <step>
     <para>
     Click <guimenu>Launch</guimenu> to start the cluster. Finishing this task
     will take some time. After it has been completely started, you will see
     the  <guimenu>Create Complete</guimenu> message. Navigate to the instance
     overview page for the &admin_node; via
     <menuchoice><guimenu>Project</guimenu> <guimenu>Compute</guimenu>
     <guimenu>Instances</guimenu></menuchoice>.
     </para>
     <informalfigure>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="horizon_stack_resources.png" width="100%"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="horizon_stack_resources.png" width="100%"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
    <step>
     <para>
      Locate the external IP address for the &admin_node; under the <guimenu>IP
      Addresses</guimenu> headline.  Visit that IP address in your browser to
      open the &admin_node; dashboard. Continue with <xref
      linkend="sec.deploy.nodes.admin_configuration"/>.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec.deploy.preparation.openstack.cli">
   <title>Launching the Stack Using the &ostack; CLI</title>

   <para>
    To get access to the &ostack; command-line tools you can either access them
    by logging in on your &soc; admin server via <literal>ssh</literal> or by
     <link xlink:href="https://www.suse.com/documentation/suse-openstack-cloud-7/book_cloud_user/data/sect1_13_chapter_book_cloud_user.html">
     installing a local <command>openstack</command> client</link>.
   </para>
   <para>
    To use the local client, you first need to access the Horizon Dashboard and
    go to <menuchoice><guimenu>Project</guimenu>
     <guimenu>Compute</guimenu> <guimenu>Access &amp;
     Security</guimenu></menuchoice> in the Horizon Dashboard and click
     <guimenu>Download &ostack; RC File v3</guimenu>.
   </para>
   <para>
    The downloaded file is a script that you then need to load using the
    <command>source</command> command. The script will ask you for your &soc;
    password.
   </para>
   <screen>&prompt.user;source container-openrc.sh</screen>

   <procedure>
    <step>
     <para>
      Adjust the <filename>caasp-environment.yaml</filename> according to your
      needs.  You should at least adjust the &rootuser; password and the
      number of worker nodes that should be launched. Also make sure the
      networking data is correct.
     </para>
     <warning>
      <title>Provide a Secure &rootuser; Password</title>
      <para>
       The &rootuser; password is set to <literal>linux</literal> by
       default. It is strongly recommended to replace it with a secure
       password.
      </para>
     </warning>
    </step>
    <step>
     <para>
      Create and launch the stack using the <command>openstack stack
      create</command> command:
     </para>
     <screen>&prompt.user;<command>openstack stack create -t caasp-stack.yaml \
-e caasp-environment.yaml --parameter image=<replaceable>CaaSP-3</replaceable>
      <replaceable>caasp3-stack</replaceable></command></screen>
     <para>
      The options <option>-t</option> and <option>-e</option> take the
      parameters for the template and environment file. With
      <option>image</option> you specify the name for the &productname;
      image. The last parameter is the name for the stack.
     </para>
    </step>
    <step>
     <para>
      Locate the external IP address for the &admin_node; by running the
      following command:
     </para>
     <screen>&prompt.user;openstack server list --name "admin" | awk 'FNR > 3 {print $4 $5 $9}'
caasp3-stack-admin|10.81.1.51</screen>
     <para>
      Visit that IP address in your browser to open the &admin_node;
      dashboard. Continue with <xref
      linkend="sec.deploy.nodes.admin_configuration"/>.
     </para>
    </step>
   </procedure>
  </sect2>
 </sect1>

 <sect1 xml:id="sec.deploy.preparation.public_cloud">
  <title>Preparing a Public Cloud Installation</title>
  <para>
   The &productname; images published by &suse; are located in the General
   Catalog in Amazon EC2, the Marketplace in Microsoft Azure, and are
   launch-able from the suse-byos-cloud project in Google Compute Engine. They
   can be found using <command>pint</command>, see <link
   xlink:href="https://www.suse.com/c/riddle-me-this/"/> for more information.
  </para>
  <para>
   The images are provided as <literal>Bring Your Own Subscription
   (BYOS)</literal> images. &productname; instances need to be registered with
   the &scc; in order to receive bugfix and security updates. Images labeled
   with the <literal>cluster</literal> designation in the name are not intended
   to be started directly; they are deployed by the &admin_node;. The
   &admin_node; image contains the <literal>admin</literal> designation in the
   image name.
  </para>

  <important>
   <title>Requirements</title>
   <para>
    Before you start provisioning the nodes, make sure to read the system
    requirements for public cloud installations in <xref
    linkend="sec.deploy.requirements.environment.public_cloud"/>.
   </para>
  </important>

  <sect2 xml:id="sec.deploy.preparation.public_cloud.provisioning.aws">
   <title>Preparing a Amazon Web Services EC2 Installation</title>
   <para>
    To prepare a Amazon EC2 installation follow these steps:
   </para>
   <procedure>
    <step>
     <para>
      Select proper instance types. You may select from one of the predefined
      instance types, hand select for general container workloads, or choose
      <guimenu>Other types...</guimenu> and enter any <guimenu>instance
      type</guimenu>, defined at <link
      xlink:href="https://aws.amazon.com/ec2/instance-types/"/>.
     </para>
    </step>
    <step>
     <para>
      Two configuration options are required in EC2:
     </para>
     <variablelist>
      <varlistentry>
       <term>Subnet ID</term>
       <listitem>
        <para>
         The <literal>subnet</literal> within which cluster nodes will be
         attached to the network, in the form
         <literal>subnet-xxxxxxxx</literal>.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>Security Group ID</term>
       <listitem>
        <para>
         The <literal>security group</literal> defining network access rules
         for the cluster nodes, in the form <literal>sg-xxxxxxxx</literal>.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
     <para>
      The defaults used for those two options are preset to the subnet ID of
      the &admin_node; and the security group ID that will be automatically
      created by <command>caasp-admin-setup</command>. You may choose to place
      the cluster nodes in a different subnet and may also use a custom
      security group. If you do so, make sure that traffic is allowed between
      the individual cluster nodes and also between the &admin_node; and all
      other cluster nodes.
     </para>
     <para>
      See the <link xlink:href="https://aws.amazon.com/documentation/vpc/">
      Amazon Virtual Private Cloud Documentation</link>for more information.
     </para>
    </step>
    <step>
     <para>
      Deploy the &admin_node; by running <command>caasp-admin-setup</command>.
      For details, see <xref
      linkend="sec.deploy.nodes.admin_install_cli" />.
     </para>
    </step>
    <step>
     <para>
      Finish the installation by bootstrapping the nodes. For details, see
      <xref linkend= "sec.deploy.install.bootstrap" />.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec.deploy.preparation.public_cloud.provisioning.azure">
   <title>Preparing a Microsoft Azure Installation</title>
   <para>
    To prepare a Microsoft Azure installation follow these steps:
   </para>
   <procedure>
    <step>
     <para>
      You need to configure credentials for access to the Azure framework so
      instances can be created, as well as parameters for the cluster node
      instances themselves. The credentials refer to authentication via a
      service principal. See
     <link xlink:href="https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-group-create-service-principal-portal"/>
     for more information on how you can create a service principal.
     </para>
     <variablelist>
      <varlistentry>
       <term>Subscription ID</term>
       <listitem>
        <para>
         The subscription ID of your Azure account.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>Tenant ID</term>
       <listitem>
        <para>
         The tenant ID of your service principal, also known as the Active
         Directory ID.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>Application ID</term>
       <listitem>
        <para>
         The application ID of your service principal.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>Client Secret</term>
       <listitem>
        <para>
         The key value or password of your service principal.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
     <para>
      Below the <guimenu>Service Principal Authentication</guimenu> box you
      will find the <guimenu>Instance Type</guimenu> configuration. You may
      select from one of the predefined instance types, hand selected for
      general container workloads, or choose <guimenu>Other types...</guimenu>
      and enter any <literal>size</literal>, as defined at <link
      xlink:href="https://docs.microsoft.com/en-us/azure/virtual-machines/linux/sizes/"/>.
      Set the <guimenu>Cluster size</guimenu> using the slider.
     </para>
     <para>
      The parameters in <guimenu>Resource Scopes</guimenu> define attributes of
      the cluster instances, as required for Azure Resource Manager:
     </para>
     <variablelist>
      <varlistentry>
       <term>Resource Group</term>
       <listitem>
        <para>
         The Resource Group in which all cluster nodes will be created.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>Storage Account</term>
       <listitem>
        <para>
         The Storage Account that will be used for storing the cluster node OS
         disks. See
         <link
           xlink:href="https://docs.microsoft.com/en-us/azure/storage/common/storage-create-storage-account"/>
         for more information about Azure Storage Accounts.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>Network</term>
       <listitem>
        <para>
         The virtual network the cluster nodes will be connected to.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>Subnet</term>
       <listitem>
        <para>
         A subnet in the previously defined virtual network. See
         <link xlink:href="https://docs.microsoft.com/en-us/azure/virtual-network/"/>
         for more information about Azure Virtual Networks.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
    </step>
    <step>
     <para>
      Deploy the &admin_node; by running <command>caasp-admin-setup</command>.
      For details, see <xref
      linkend="sec.deploy.nodes.admin_install_cli" />.
     </para>
    </step>
    <step>
     <para>
      Finish the installation by bootstrapping the nodes. For details, see
      <xref linkend= "sec.deploy.install.bootstrap" />.
     </para>
    </step>
   </procedure>
  </sect2>
  <sect2 xml:id="sec.deploy.preparation.public_cloud.provisioning.google">
   <title>Preparing a Google Compute Engine Installation</title>
   <para>
    To prepare a Google Compute Engine (GCE) installation follow these steps:
   </para>

   <procedure>
    <step>
     <para>
      Select proper instance types. You may select from one of the predefined
      instance types, hand select for general container workloads, or choose
      <guimenu>Other types...</guimenu> and enter any <guimenu>machine
      type</guimenu>, defined at <link
      xlink:href="https://cloud.google.com/compute/docs/machine-types/"/>.
     </para>
    </step>
    <step>
     <para>
      Two configuration options are required in GCE:
     </para>
     <variablelist>
      <varlistentry>
       <term>Network</term>
       <listitem>
        <para>
         The name of the virtual network the cluster nodes will run within.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>Subnet</term>
       <listitem>
        <para>
         If you created a custom network, you must specify the name of the
         subnet within which the cluster nodes will run.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
     <para>
      See the <link xlink:href="https://cloud.google.com/vpc/docs/vpc">GCE
      Network Documentation</link> for more information.
     </para>
    </step>
    <step>
     <para>
      Deploy the &admin_node; by running <command>caasp-admin-setup</command>.
      For details, see <xref
      linkend="sec.deploy.nodes.admin_install_cli" />.
     </para>
    </step>
    <step>
     <para>
      Finish the installation by bootstrapping the nodes. For details, see
      <xref linkend= "sec.deploy.install.bootstrap" />.
     </para>
    </step>
   </procedure>
  </sect2>
 </sect1>
</chapter>
